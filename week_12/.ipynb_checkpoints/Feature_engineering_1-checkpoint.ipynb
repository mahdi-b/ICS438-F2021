{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2575f915",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Some) Problem with Approach used in Assignment 1\n",
    "\n",
    "* Lack of overlap between a large collection of documents\n",
    "  * The more disparate documents we consider, the smaller the intersection\n",
    "  * the intersection is actually very bad for clustering documents\n",
    "\n",
    "* Considering all the non-stop words shared between the documents (union) leads to an unnecessarily lot of very large datasets\n",
    "  * Many words that share a prefix, e.g., leave, leaving, left all lead to different counts despite potentially referencing the same concept\n",
    "* * The semantic meaning of words is not considered\n",
    "  * We want the words water in the following sentences to have different meanings and hence counts\n",
    "     * \"Let anyone who wishes take the water of life as a gift.\" and \"The water cycle is the continuous circulation of water in the Earth’s atmosphere.\"\n",
    "  * We need to take the semantic meaning of the word into account when counting\n",
    "  * This is (was?) a *very hard* problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5affc5cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distribution of Words in a Text \n",
    "\n",
    "* The frequency distribution of words in a language follows Zipf's law\n",
    "  * Just and FYI: this makes computign statistics rather difficult or impossible\n",
    "  \n",
    "![](https://www.dropbox.com/s/neydq8wi2kqqof3/zipf_law.png?dl=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6411d3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document Similarity: A Matching Score\n",
    "\n",
    "* Document Search in Information Retrieval faces the same issue\n",
    "\n",
    "  * A field with a long history of creative solutions to similar problems\n",
    "  * Finding similar documents in a corpus\n",
    "  * Ideally, we want the search to rank the hits (documents found) by their similarity to the query  \n",
    "  We wish to return in order the documents most likely to be useful to the searcher\n",
    "\n",
    "* How can we rank-order the documents in the collection with respect to a query?\n",
    "  * Assign a score – say in [0, 1] – to each document\n",
    "  * This score measures how well hit and query “match”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee4d13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Query-hit matching scores\n",
    "\n",
    "* Consider a one-term query\n",
    " If the query term does not occur in the document, score should be 0\n",
    "* The more frequent the query term in the document, the higher the score (should be)\n",
    "   * We will look at a number of alternatives for this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381d0a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jaccard Coefficient\n",
    "\n",
    "\n",
    "* Recall that the Jaccard Coefficient is a commonly used measure of overlap between two sets A and B\n",
    "  * The number of overlaps between A and B normalized by all the words in A and B.\n",
    "* Does not require A and B to have the same size.\n",
    "* Always assigns a number between 0 and 1.\n",
    "\n",
    "* Shortcoming:\n",
    "* Does not consider term frequency\n",
    "* Jaccard doesn’t consider the fact that rare terms in a collection are more informative than frequent terms. \n",
    "  * The reason why the intersection is not a good idea\n",
    "\n",
    "* We need a more sophisticated way of normalizing for length (instead of $|A \\cup B|$)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804f50a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Term-document Count Matrices\n",
    "\n",
    "* In a count matrix, each word is represented with its frequency (count in a document)\n",
    "  * As was mentioned in Assignment 2, this is called the bag of words model \n",
    "* Does not consider the order of words in the document\n",
    "\n",
    "* `John is quicker than Mary` and `Mary is quicker than John` have the exact same vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e47cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term frequency `tf`\n",
    "\n",
    "* Formally, the term frequency $tf_{t,d}$ of term $t$ in document $d$ is defined as the number of times that t occurs in d.\n",
    "*  We want to use $tf$ when computing query-document match scores. \n",
    "* A document with 10 occurrences of the term is more relevant than a document with 1 occurrence of the term.\n",
    "  * But not 10 times more relevant.\n",
    "* Relevance does not increase proportionally with term frequency.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eddaa8f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log-frequency weighting\n",
    "\n",
    "* The log-frequency weight of term $t$ in $d$ is\n",
    "\n",
    "$$\n",
    "w_{t,d} = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1+\\log_{10}\\mbox{tf}_{t,d} & \\mbox{if } {tf}_{t,d} > 0\\\\\n",
    "        0 & \\mbox{otherwise}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "```0 → 0, 1 → 1, 2 → 1.3, 10 → 2, 1000 → 4, etc.```\n",
    "\n",
    "* Score for a document-query pair: sum over terms t in both q and d:\n",
    "\n",
    "$$\n",
    "\\mbox{score} = \\sum_{t\\in A\\cap B}(1+\\log_{10}\\mbox{tf}_{t,d})\n",
    "$$\n",
    "\n",
    "The score is 0 if none of the query terms is present in the document.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdc427",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document frequency\n",
    "\n",
    "* We still have the issue of rare terms\n",
    "  * Rare terms are more informative than frequent terms\n",
    "    * Recall stop words\n",
    "* Consider a term in the query that is rare in the collection (e.g., arachnocentric)\n",
    "  * A document containing this term is very likely to be relevant to the query arachnocentric\n",
    "  * The term is very likely to be relevant to clustering the docuements  \n",
    "  * Thus, we want a high weight for rare terms like arachnocentric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b660d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document frequency, continued\n",
    "\n",
    "* Frequent terms are less informative than rare terms.\n",
    "* Consider a query term that is frequent in the collection (e.g., `high`, `increase`, `true`)\n",
    "  * A document containing such a term is more likely to be relevant than a document that doesn’t\n",
    "  * But it’s not a sure indicator of relevance.\n",
    "* For frequent terms, we want:\n",
    "  * High positive weights for words like `high`, `increase`, and `true`\n",
    "  * But lower weights than for rare terms.\n",
    "* We will use document frequency (`df`) to capture this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beb188b1",
   "metadata": {},
   "source": [
    "### `idf` Weight\n",
    "\n",
    "$df_t$ is the document frequency of $t$\n",
    "  * The number of documents that contain $t$\n",
    "* $df_t$ is an inverse measure of the informativeness of $t$\n",
    "    * $df_t \\le N$\n",
    "\n",
    "We define the $idf$ (inverse document frequency) of $t$ by\n",
    "$$\n",
    "idf_t = log_{10}(N/df_t)\n",
    "$$\n",
    "* We use the `log` to dampen the effect of idf\n",
    "  * Particularly useful when we have a large number of documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e7e5c",
   "metadata": {},
   "source": [
    "### `idf` Weight\n",
    "\n",
    "$df_t$ is the document frequency of $t$\n",
    "  * The number of documents that contain $t$\n",
    "* $df_t$ is an inverse measure of the informativeness of $t$\n",
    "    * $df_t \\le N$\n",
    "\n",
    "We define the $idf$ (inverse document frequency) of $t$ by\n",
    "$$\n",
    "idf_t = log_{10}(N/df_t)\n",
    "$$\n",
    "* We use the `log` to dampen the effect of `idf`\n",
    "  * Particularly useful when we have a large number of documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db5a3046",
   "metadata": {},
   "source": [
    "### `tf-idf` Weighting\n",
    "\n",
    "* The `tf-idf` weight of a term is the product of its `tf` weight and its `idf` weight.\n",
    "\n",
    "$$\n",
    "w_{t,d} = log(1+tf_{t,d}) \\times log(N/df_t)\n",
    "$$\n",
    "\n",
    "* A popular weighting scheme in information retrieval\n",
    "  * Alternative names: tf.idf, tf x idf\n",
    "\n",
    "* Increases with the number of occurrences within a document\n",
    "* Increases with the rarity of the term in the collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d3c162a",
   "metadata": {},
   "source": [
    "### Score for a Document Given a Query\n",
    "\n",
    "$$\n",
    "Score(A, B) = \\sum_{t\\in A\\cap B} tf.idf_{t,d}\n",
    "$$\n",
    "\n",
    "* There are many variants\n",
    "  * How “tf” is computed (with/without logs)\n",
    "  * Whether the terms in the query are also weighted\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f77a47d9",
   "metadata": {},
   "source": [
    "### Using `tf-idf` for Feature Engineering\n",
    "* Each document is represented by a real-valued vector of tf-idf weights \\in R^{|V|}\n",
    "\n",
    "![](https://www.dropbox.com/s/1bx77e488ee6wek/count_tf_idf.png?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dea71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Documents as Vectors and  Space Proximity\n",
    "\n",
    "\n",
    "\n",
    "* Key idea 1: Do the same for queries: represent them as vectors in the space\n",
    "* Key idea 2: Rank documents according to their proximity to the query in this space\n",
    "  * proximity = similarity of vectors\n",
    "* First cut: distance between two points\n",
    "( = distance between the end points of the two vectors)\n",
    "* Euclidean distance is a bad idea when the instances have differet lengths\n",
    "  * The Euclidean distance is large for vectors of different lengths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f474c7a",
   "metadata": {},
   "source": [
    "### Documents as Vectors and  Space Proximity - Cont'd\n",
    "\n",
    "* Thought experiment: take a document $d$ and append it to itself. Call this document $d′$.\n",
    "  * “Semantically” $d$ and $d′$ have the same content\n",
    "The Euclidean distance between the two documents can be quite large\n",
    "\n",
    "* The angle between the two documents is 0, corresponding to maximal similarity.\n",
    "\n",
    "* Key idea: Rank documents according to angle with query.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95540e",
   "metadata": {},
   "source": [
    "### From Angles to Cosines\n",
    "\n",
    "* In informtion retrieval, the following two notions are equivalent.\n",
    "  * Rank documents in decreasing order of the angle between query and hit\n",
    "  * Rank documents in increasing order  of cosine(query,hit)\n",
    "\n",
    "* Cosine is a monotonically decreasing function for the interval [0o, 180o]\n",
    "\n",
    "![](https://www.dropbox.com/s/lpq4vvnlnmz0oxw/cosine.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ef883",
   "metadata": {},
   "source": [
    "### Length Normalization\n",
    "\n",
    "* A vector can be (length-) normalized by dividing each of its components by its length \n",
    "  * We commonly use the $L2$ norm:\n",
    "\n",
    "* Dividing a vector by its L2 norm makes it a unit (length) vector\n",
    "\n",
    "  * Effect on the two documents $d$ and $d′$ (d appended to itself) have identical vectors after length-normalization.\n",
    "  * Thus, long and short documents now have comparable weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af81a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cosine Similairity\n",
    "\n",
    "* q_i is the `tf-idf` weight of term `i` in the query\n",
    "* `d_i` is the tf-idf weight of term `i` in the document\n",
    "\n",
    "![](https://www.dropbox.com/s/4x1fb50xiqidmnf/cos_equation.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d033e6",
   "metadata": {},
   "source": [
    "### Cosine Similarity Illustrated\n",
    "\n",
    "![](https://www.dropbox.com/s/4inqt6nf9mfz6h9/cosine_similarity.png?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a644a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
