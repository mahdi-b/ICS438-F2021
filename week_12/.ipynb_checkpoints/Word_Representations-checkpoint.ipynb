{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59b72ab",
   "metadata": {},
   "source": [
    "### Information Retrieval and Document Search\n",
    "* Concerned with obtaining stored in an unstructured manner\n",
    "  * Find documents that satisfy some level of query-document similarity\n",
    "* `tf-idf` computes a \"weight\" for every word of the query.\n",
    "  * Those can be used to derive features for words in a document\n",
    "* So far, the weight is taken in a literal sense\n",
    "  * If you use the keyword notebook, documents containing the word laptop won't be identified\n",
    "* This is problematic since there are many ways synonyms can be combined to form a query\n",
    "  * A naïve information retrieval system does nothing to help\n",
    "  * We can handle stemming and case folding but cannot do more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336e466",
   "metadata": {},
   "source": [
    "### Including Word Meanings\n",
    "\n",
    "* Use information on word similarity using synonyms?\n",
    " * E.g. use a database of synonyms and include a word's popular synonyms in the query?\n",
    " * Could be a manually curated measure of word similarity. \n",
    "   * relatively easy to derive from a big collection of documents\n",
    "* Unfortunately, context-free query expansion ends up problematic\n",
    "  * [light hair] ≈ [fair hair] => expand [light] ⇒ [light fair]\n",
    "  * [outdoor light] ≠ [outdoor fair]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509903ae",
   "metadata": {},
   "source": [
    "### Synonyms versus Word Similarity\n",
    "\n",
    "* It's hypothesized that there are not perfect synonyms\n",
    "  * No two words that have the same meaning and that can be used interchangeably in all circumstances\n",
    "\n",
    " <img src=\"https://www.dropbox.com/s/nkirtpgru0nmcce/about_synonyms.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "* Gabriel Girard French churchman (Abbot) and grammarian \n",
    "  * Author of the first work on synonyms published in France\n",
    "* While there may not be perfect meaning, there are words sharing some level of meaning\n",
    " * Big/large, water/H2O, Apples/Oranges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef644089",
   "metadata": {},
   "source": [
    "### Examples of Words Similarity \n",
    "* Simlex-999 was produced by mining the opinions of 500 annotators via Amazon Mechanical Turk. \n",
    " <img src=\"https://www.dropbox.com/s/i5s3xy9h3fzapzq/simlex-999%20question.png?dl=1\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "* Often used as the gold standard resource for the evaluation of models that learn the meaning of words and concepts.\n",
    "  \n",
    "* https://fh295.github.io/simlex.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766129e6",
   "metadata": {},
   "source": [
    "### Similairty and Relatedness\n",
    "\n",
    "* Words can be related in ways that are beyond sinonymity\n",
    "* Those are called word associations or word relatedness. E.g.: \n",
    "  * Coffee and tea \n",
    "    * They are similar: both are hot beverages, produced from plants, stimulants, etc...\n",
    "  * Coffee and Mug    \n",
    "      * Are related: One is used to consume the other\n",
    "      * Are not similar: one is produced from a plant the other is manufactured, one is liquid the other is solid, etc...\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40934aa",
   "metadata": {},
   "source": [
    "### Similarity and Relatedness - Cont'd\n",
    "\n",
    "* Related words belong to the same field\n",
    "  * Medicine: doctors, surgeon, nurse, hospital, anesthetic, etc.\n",
    "  * Restaurant: menu, waiter, chef, food, drink, etc.\n",
    "  * etc.\n",
    "* Antonyms are also typically related as they describe opposite meanings of the same feature.\n",
    "  * E.g.: hot/cold, dark/light, slow/fast, etc. \n",
    "* Words with a similar connotation (sentiment)  \n",
    "  * positive connotation (happy, ecstatic, joyful, ...)\n",
    "  * negative connotation (sad, upset, down....)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5527f",
   "metadata": {},
   "source": [
    "### Connotation\n",
    "\n",
    "* FYI: Since many are working on this for the final project!\n",
    "* Words vary along 3 affective dimensions\n",
    "  * Valence: \"relative capacity to unite, react, or interact as with antigens or a biological substrate\" Webster dictionary\n",
    "    * Pleasantness of a stimulus\n",
    "    * love=1 versus nightmare=0\n",
    "        \n",
    "  * Arousal: Intensity of emotion provoked by a stimulus\n",
    "    * Elated=1 versus meditative=0\n",
    "  * Dominance: Degree of control asserted by a stimulus\n",
    "    * Leadership=1 versus weak=0\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c9101",
   "metadata": {},
   "source": [
    "### Learning Similarity\n",
    "\n",
    "* You can learn query context-specific rewritings from search logs by attempting to identify the same user making a second attempt at the same user need\n",
    "  * Foundational idea: the meaning of a word is tied to its use in the language\n",
    "    * We have tons of data that shows how words are used. How can we extract the meaning of words from the data\n",
    "\n",
    "* So far, we've encoded a document as a bag of words\n",
    "  * Words are assigned numbers based on, for example, their occurrence in a text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ffadaf",
   "metadata": {},
   "source": [
    "### Automatic Thesaurus Generation\n",
    "\n",
    "* Attempt to generate a thesaurus automatically by analyzing a collection of documents\n",
    "* Fundamental notion: similarity between two words\n",
    "* Definition 1: Two words are similar if they co-occur with similar words.\n",
    "* Definition 2: Two words are similar if they occur in a given grammatical relation with the same words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75718a",
   "metadata": {},
   "source": [
    "### Definition 1: Meaning from Linguistic Distribution\n",
    "\n",
    "* What is Ong Choi?\n",
    "\n",
    "* Given the following set of statements:\n",
    "  * Ong choi is delicious sauteed with garlic... salt\n",
    "  * I had ong choi over rice\n",
    "  * Ong choi .... leaves should be washed thoroughly\n",
    "* And the following second set of document titles\n",
    "  * Recipe for spinach with garlic and rice\n",
    "  * Garlicky Swiss Chard Recipe on the NYT .. garlic and rice \n",
    "* From the above, we can hypothesize that:\n",
    " * Ong choi is an edible vegetable that is **similar** to Spinach and Chard greens?\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a940d068",
   "metadata": {},
   "source": [
    "### Definition 2: Meaning as a point in Space\n",
    "\n",
    "* We can use data as a point in space\n",
    "  * each dimension describes the value along a given axis\n",
    "    * e.g.: scores for valence, arousal, and dominance.\n",
    "        \n",
    "* We can alternative or even custom features that are domain-specific\n",
    "  * For instance, axes can be tool-like, size-like, transportation-specific, etc...\n",
    "    \n",
    "* We call this an embedding\n",
    "  * Vector representation such that similar words have a similar representation in higher-dimensional pace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd183383",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "* Ideally, we would prefer a method that combines both approaches\n",
    "  * Words that have similar linguistic distribution have similar embeddings\n",
    "* We want the embedding to be easily learned\n",
    " * We don't want to have to define the axis and score each word on its value for a specific axis\n",
    "* Word embeddings are the standard way to represent words in modern Natural Language Processing (NLP) applications\n",
    "* Embeddings allow us to compare documents based on \"similar\" words\n",
    "  * As opposed to the exact same words, as done in Assignment 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00691bb",
   "metadata": {},
   "source": [
    "### Docuement Representaiton in Terms of Words\n",
    "* Representaiton used in Assignment 2\n",
    " <img src=\"https://www.dropbox.com/s/lh2afrz6p1l6a5t/doc_word_representation.png?dl=1\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a128d",
   "metadata": {},
   "source": [
    "### Document Representation in Terms of Words\n",
    "\n",
    "* But we can also choose a representation of words based on which document they occur in.\n",
    "\n",
    " <img src=\"https://www.dropbox.com/s/wdwjexi30f8txqa/word_doc_representation.png?dl=1\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    " \n",
    "* Over a very large collection of documents, similar words will have similar vectors because they co-occur in the same documents\n",
    "  * this, at least intuitively, satisfying ideas 1 and 2\n",
    "\n",
    "* Instead of working with document word matrix, we can work with document matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f1325",
   "metadata": {},
   "source": [
    "### Word-Word Matrix (Word-context)\n",
    "* Instead of working document word matrix, we can work with a word-word matrix\n",
    " <img src=\"https://www.dropbox.com/s/wanm7n5uvw772dt/word-word_matrix.png?dl=1\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "* Each cell represents the number of times a $w_i$ and $w_j$ were observed in the same context.\n",
    "  * context is not limited to documents. \n",
    "    * Can be a paragraph or a sentence or even a window of custom size (5 words to the right or left)\n",
    "    \n",
    "* These are called term-context matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa8219",
   "metadata": {},
   "source": [
    "### Word-Context Example\n",
    "\n",
    " <img src=\"https://www.dropbox.com/s/csqlw32a4sqku6a/word_context_example.png?dl=1\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "* Note that these vectors are extremely large and sparse\n",
    "* Appropriate data structures need to be used to store and compute on these matrices\n",
    "\n",
    "* Use Cosine similarity to measure distance between words\n",
    "  * Euclidean distance is not appropriate for the same reasons discussed in the td-idf section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35644d",
   "metadata": {},
   "source": [
    "### Weighting to Mitigate Overly Frequent Words\n",
    "\n",
    "* The frequencies measured are raw. \n",
    "* Words like `raw` and `vegetable` will have a low frequency compared to `a` and `the`\n",
    "* While frequency is useful, we want to make the meaning of frequent words less meaningful \n",
    "* Two solutions:\n",
    "  * tf-idf: instead of frequencies compute the tf-idf values instead\n",
    "    * Important to rememebr that here, document is the context.\n",
    "    * Document can be a paragraph or even a sentence\n",
    "  * point-wise mutual information: Instead of frequency compute the probability of randomly observing the two words based their frequency.\n",
    "$$\n",
    "  PMI = \\log\\frac{p(w_1, w_2)}{p(w1) \\times p(w_2))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3fb90",
   "metadata": {},
   "source": [
    "### Word2Vec Embedding Method\n",
    "\n",
    "* Word2Vec content was moved to its own notebook in `Week 13`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f9102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
