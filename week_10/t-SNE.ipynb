{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### t-SNE: a Brief Introduction\n",
    "\n",
    "* t-SNE, or t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique that is particularly well suited for the visualization of high-dimensional datasets \n",
    "  * Ideal for plotting things down to two dimensions\n",
    "\n",
    "* Like PCA, t-SNE takes a high dimensional dataset and reduces it to two dimensions.\n",
    "\n",
    "  * Emphasizes retaining the \"neighborhoods.\"\n",
    "    * No guarantees about very distant points\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://www.dropbox.com/s/lw7sy9xda3pfoaj/t-SNE_Intro.png?dl=1)\n",
    "\n",
    "\n",
    "* Observe that there is no clear axis on which we can project while maintaining the variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### t-SNE Key Idea\n",
    "\n",
    "* Prior to t-SNE, the was SNE (Stochastic Neighbor Embedding)\n",
    "\n",
    "* SNE preserves the distances between the nearest neighbors in high-dimensional space by making them nearest neighbors in low-dimensional space.\n",
    " \n",
    "  * this is an approach that is common to many non-linear dimensionality reductions \n",
    "    * Unlike PCA, not concerned by the variance (distance between faraway points)\n",
    "    \n",
    "  * Makes them ideal and robust for highlighting clusters similarity in lower-dim space\n",
    "  \n",
    "* Issue of crowding\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://www.dropbox.com/s/u0q26ntkj1gata4/shuffle_data.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://www.dropbox.com/s/tk1yxknnszl0ewd/t-SNE_intuition.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  SNE Key Idea - Cont'd\n",
    "\n",
    "* SNE models similarity between instances using probability\n",
    "  * Converts the Euclidean distance into a probability\n",
    "  * You can think of it as the probability that $x_i$ and $x_j$ are actually neighbors\n",
    "    * Probability is easier to work with here because distance does not necessarily imply proximity in lower-dimensional space.\n",
    "    \n",
    "    \n",
    "* The similarity between points of the original dataset $x_j$ and $x_i$ is the conditional probability of picking $x_j$ having selected $x_i$, \n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/cli98l323uq125b/dist_xi_xj.png?dl=1\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "\n",
    "* This is, somewhat similar to weight computed from the k-Nearest Neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SNE Key Idea - Cont'd\n",
    "\n",
    "* In the low dimensional space (ex. $d=2$) we define the distance between the points $i$ and $j$ as\n",
    "\n",
    "$$\n",
    "q_{j|i} = \\frac{e^{-|y_i -y_j|^2}}{\\sum_{k \\ne i} e^{-|y_i -y_k|^2}}\n",
    "$$\n",
    "\n",
    "* This is similar to the distance in the high-dimensional space, except that variance is constant and the same across all the points.\n",
    "\n",
    "* Ideally, we want to preserve distance as much as possible such $p_{i,j}$ an $q_{i,j}$ are as close as possible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SNE Key Idea - Cont'd\n",
    "\n",
    "\n",
    "* We want to reconcile the distances in high-dim space and low-dimensional space. \n",
    "  * We want to infer a new distance in space $y$. \n",
    "  * Which distances should we pick so that distances in low-dimensional space make sense?\n",
    "\n",
    "* We propose a cost function that is used to minimize the distances in both coordinate systems.\n",
    "  * We use the KL divergence for that.\n",
    "  \n",
    "  \n",
    "$$\n",
    "    KL(p||q) \\sum_{ij} p_{j|i} log\\frac{p_{j|i}}{q_{j|i}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNE Key Idea - Cont'd\n",
    "\n",
    "* Kullbackâ€“Leibler divergence is simply a measure of how one probability distribution is different from a second, reference probability distribution\n",
    "\n",
    "* What we need to do is find the sets of y_{ij} which minimize the KL divergence.\n",
    "  * Those are the distances in 2D space.\n",
    "    \n",
    "![](https://www.dropbox.com/s/0zvh0avtsasszpr/KL_divergence.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Does this work?\n",
    "\n",
    "$$\n",
    "    KL(p||q) \\sum_{ij} p_{j|i} log\\frac{p_{j|i}}{q_{j|i}}\n",
    "$$\n",
    "\n",
    "* Recall that: \n",
    "  * $p_{j|i}$ is close to 1 if $i$ is very close to $j$ \n",
    "  * $p_{j|i}$ is close to 0 if $i$ is very distant from $j$ \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Does this work?\n",
    "\n",
    "$$\n",
    "    KL(p||q) \\sum_{ij} p_{j|i} log\\frac{p_{j|i}}{q_{j|i}}\n",
    "$$\n",
    "\n",
    "* If $p_{j|i}$ is close to 1 and $q_{j|i}$ is close to 1, then  $KL(p||q)$ is close to 0\n",
    "   * This generalizes to  $p_{j|i} \\approx q_{j|i}$  \n",
    "\n",
    "\n",
    "\n",
    "* If $p_{j|i}$ is large and $q_{j|i}$ small, then  $KL(p||q)$ is high\n",
    "  * That is not a good solution\n",
    "\n",
    "\n",
    "\n",
    "* If $p_{j|i}$ is small and $q_{j|i}$ large, then  $KL(p||q)$ small\n",
    "  * It's okay to put two points close to each other in 2-D, although they were distant in high dim.\n",
    "  \n",
    "* SNE focuses on preserving the local structure of the data  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How is t-SNE different from SNE?\n",
    "\n",
    "1 Turning $p_{j|i}$ into a symmetric probability makes it easier to compute\n",
    "  * No need to compute $p_{j|i}$ and $p_{i|j}$\n",
    "  * this can be easily achieved by assuming that $\\sigma$ is constant in higher dimensional space.\n",
    "  \n",
    "$$  \n",
    "p_{ij} = \\frac{e^\\frac{{-|x_i -x_j|^2}}{2\\sigma^2}}{\\sum_{k \\ne i} e^\\frac{{-|x_i - x_k|^2}}{2\\sigma^2}}\n",
    "$$\n",
    "  \n",
    "2. Change the distribution in the low dim. space\n",
    "  * This is necessary due to the crowding problem (point tend to cluster in a much smaller area in low dimensional space)\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{ \\frac{1}{1+|y_i-y_j|^2}}{\\sum_{k\\ne i}\\frac{1}{1+|y_i-y_j|^2}}\n",
    "$$\n",
    "  \n",
    "  \n",
    "* Uses a $t$-distribution, rather than Gaussian\n",
    "  * More value to distant points spread out the area in 2-d space    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advantages and Limitation\n",
    "\n",
    "\n",
    "\n",
    "* Limitations\n",
    "  * Quadratic run time; Not scalable for extremely large datasets\n",
    "    * There exist variations that can handle large datasets \n",
    "    \n",
    "  * The KL-Divergence function we are trying to minimize is not convex\n",
    "    * no guarantees as to the resulting solutions\n",
    "  * Also probabilistic and can yield differnet solutions with ech run.\n",
    "  \n",
    "  * Good visualization may require tweaking the param \n",
    "    * The perplexity, which is related to the sigma used computing the distnace\n",
    "    * Number of iterations: how log to keep tweaking $q$ to make it simialr to $p$\n",
    "  * Does not transform the space: geenrate an embedding into a new space\n",
    "  * Concerned with local structure, as opposed to maintaining faraway point far.\n",
    "    \n",
    "* Advantages\n",
    "  * Works impressively well, yielding to stunning representations in lowe dimensional space\n",
    "  * Yields, in most cases, less crowded representations of the data\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
