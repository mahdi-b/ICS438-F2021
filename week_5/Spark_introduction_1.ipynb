{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac85279",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "* Apache Spark is an open-source, distributed processing system used for big data workloads.\n",
    "\n",
    "\n",
    "* It's an enhancement to Hadoop's MapReduce.\n",
    "  * Processes and retains data in memory for subsequent steps,\n",
    "  * For smaller workloads, Sparkâ€™s data processing speeds are up to 100x faster than Hadoop's MapReduce.\n",
    "\n",
    "* Spark was written in Scala\n",
    "  * Runs on the JVM\n",
    " \n",
    "* Includes libraries to support SQL queries, machine learning (MLlib), graph data analysis (GraphX) and streaming data analysis.\n",
    "  * Tools for pipeline construction and evaluation\n",
    "\n",
    "* Enahanced security.\n",
    "\n",
    "* Ideal for real-time processing as it utilizes in-memory caching and optimized query execution for fast queries against data of any size.  \n",
    "\n",
    "* Provides more operators other than map and reduce\n",
    "  * Plethora of functions for SQL-like operation, ML and working with graph data\n",
    "  * Over 80 high level operators beyond Map and Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bae26",
   "metadata": {},
   "source": [
    "### Spark and Function Programming\n",
    "\n",
    "* Spark uses functional programming to manipulate data\n",
    "  * Pradigm used in some popular languages such as Common Lisp, Scheme and Clojure, OCaml and Haskell\n",
    "  \n",
    "* Functional programming is a data oriented paradigm.\n",
    "  * Functional programming decomposes a problem into a set of functions.\n",
    "  * Logic is implemented by applying and composing functions.\n",
    "\n",
    "* It's centered around the fact that data should be manipulated by functions without maintaining any external state.\n",
    "  * No global variables\n",
    "\n",
    "  * The above defines exactly what lambda functions do.\n",
    " \n",
    "* In functional programming, we need to always return new data instead of manipulating the data in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32a78f",
   "metadata": {},
   "source": [
    "### Spark Operations and RDDs\n",
    "\n",
    "* Spark Provides its own distributed data framework called resilient distributed datasets or RDDs.\n",
    "   * RDD is an abstraction that represents a read-only collection of objects that are partitioned across machines.   \n",
    "  * RDDs are fault tolerant and are accessed via parallel operations.\n",
    "\n",
    "* RDDs are cached in memory, making it efficient to iterate ont he same data\n",
    "  * Ideal for operations such as optimization or some ML algorithms\n",
    "  * Fast operation speed makes it ideal for command-line-based queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31893de1",
   "metadata": {},
   "source": [
    "### Core Components of Spark\n",
    "\n",
    "![](https://www.dropbox.com/s/azebxe8nv5nsqne/spark_architecture.png?dl=1)\n",
    "\n",
    "* **Spark Core**\n",
    "  * Basic functionality:\n",
    "    * APIs that define RDDs\n",
    "    * operations and actions to process RDDs\n",
    "    \n",
    "* **Spark SQL**\n",
    "    * APIs to interact with Apache Hive's variant of SQL called Hive Query Language (HiveQL).\n",
    "    * DB tables are RDD and Spark SQL queries are transformed into Spark operations\n",
    "\n",
    "* **Spark Streaming**\n",
    "    * Enables the processing and manipulation of live streams of data.\n",
    "    \n",
    "* **MLlib**\n",
    "  * Implementation of machine learning algorithms using Spark on RDDs\n",
    "  * Basic algorithms for classifications, regressions,\n",
    "\n",
    "* **GraphX\n",
    "  * Functionality for manipulating graphs and performing parallel graph operations and computations\n",
    "  * A sort of large-scale Neo4J\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bd2bd",
   "metadata": {},
   "source": [
    "### Spark Paradigm\n",
    "\n",
    "A Spark program typically follows a simple paradigm:\n",
    "\n",
    "\n",
    "![](https://www.dropbox.com/s/do918x5bpeh8oh4/cluster_mode.png?dl=1)\n",
    "\n",
    "\n",
    "1. The main program is the `driver`\n",
    "2. The program has one or more workers, called executors,\n",
    "  * Those run code sent to them by the driver on their partitions of the RDD\n",
    "3. Results are then sent back to the driver for aggregation or compilation.\n",
    "\n",
    "* in local mode (or Spark in-process), the executor is the same at the driver (my laptop here)\n",
    "\n",
    "* See the cluster mode document for more details.\n",
    "\n",
    "https://spark.apache.org/docs/latest/cluster-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090af93",
   "metadata": {},
   "source": [
    "### Setting a Docker Cluster\n",
    "\n",
    "* Manually installing Spark and all its components can be a daunting task.\n",
    " * Manually deploying, configure and optimizing a Spark is complex and time consuming.\n",
    "* Easy to use Docker to install locally\n",
    "  * We will use the following Docker image\n",
    "jupyter/all-spark-notebook\n",
    "* There are other docker images, including (jupyter/pyspark-notebook), which does not includ the jobs dashboard `http://localhost:4040`\n",
    "\n",
    "```\n",
    "docker run --rm -p 4040:4040 -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "* This configuration created a master and compute nodes locally in a docker instance\n",
    " \n",
    "* While you're probably not going to need to, you can log into the running container using: `docker exec -it <CONTAINER_ID> bash`\n",
    "\n",
    "where <CONTAINER_ID> of the container currently running the `jupyter/all-spark-notebook` image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659f28e",
   "metadata": {},
   "source": [
    "### Spark Execution using PySpark\n",
    "\n",
    "\n",
    "* Interact with the Scala Interface using the PySpark Python library\n",
    "  * Wrapper that uses almost exactly the same function and attribute names\n",
    "\n",
    "The Docker instance has all the libraries installed and ready to go.\n",
    "\n",
    "* Make sure you run a Jupyer notebook on the Docker instnace\n",
    "  * If the code below fails, this means you're not running in the Docker instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8309122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 22:28:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "# sc.stop()\n",
    "sc = SparkContext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47b7cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f872e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version is 3.1.2\n",
      "Phthon version is 3.9\n",
      "The name of the master is local[*]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version is {sc.version}\")\n",
    "\n",
    "print(f\"Phthon version is {sc.pythonVer}\")\n",
    "\n",
    "print(f\"The name of the master is {sc.master}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f0394f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1632265750213'),\n",
       " ('spark.driver.port', '36283'),\n",
       " ('spark.app.id', 'local-1632265751791'),\n",
       " ('spark.driver.host', '804949680baf'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab254578",
   "metadata": {},
   "source": [
    "### Creating a Text RDD\n",
    "\n",
    "* You can create RDDs in a number of ways:\n",
    "\n",
    "    * `parallelize()`: function to transform Python collections (list-like data structures) into RDDs\n",
    "    * Distributes the passed list and makes it fault-tolerant.\n",
    "\n",
    "* Another easy way to create RDDs is to read in a file with `textFile()`\n",
    "\n",
    "* Creates an RDD where every object is a line of the input text file\n",
    "\n",
    "* Once an RDD is created, we can access its `map`, `reduce` and `filter` methods\n",
    " * Those operations and other we will cover are called 'transformations'\n",
    " * A transformation on a RDD yields a new RDD\n",
    " * `flatMap` is also commonly used and is equivalent to `itertools.chain()`\n",
    " \n",
    "* To get (see) results, we need to perform an 'action' on an RDD\n",
    "  * Some actions like `take()`, `takeSample` and `count()`, `mean()` etc., are needed to return data.\n",
    " \n",
    "* More on the distinction between transformations and actions later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78163e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: randomuser in /opt/conda/lib/python3.9/site-packages (1.6)\n",
      "user object  is <randomuser.RandomUser object at 0x7f0230456910>\n",
      "user json representation  is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_id': '468-88-4219',\n",
       " 'first_name': 'Aubree',\n",
       " 'last_name': 'Hunt',\n",
       " 'state': 'Ohio',\n",
       " 'zip': 45039,\n",
       " 'lat_long': {'latitude': '55.0586', 'longitude': '13.8443'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insstall using the following if not already installed \n",
    "!pip install randomuser\n",
    "from randomuser import RandomUser\n",
    "\n",
    "# Generate a single user\n",
    "user = RandomUser({\"nat\": \"us\"})\n",
    "print(f\"user object  is {user}\")\n",
    "def get_user_info(u):\n",
    "\n",
    "    user_dict = {\n",
    "        \"user_id\": u.get_id()[\"number\"], \n",
    "        \"first_name\": u.get_first_name(), \n",
    "        \"last_name\": u.get_last_name(), \n",
    "        \"state\": u.get_state(),\n",
    "        \"zip\": u.get_zipcode(),\n",
    "        \"lat_long\": u.get_coordinates()\n",
    "    }\n",
    "    return user_dict\n",
    "\n",
    "user_json = get_user_info(user)\n",
    "print(f\"user json representation  is\")\n",
    "user_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9cd1a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<randomuser.RandomUser at 0x7f02304560d0>,\n",
       " <randomuser.RandomUser at 0x7f0230456610>,\n",
       " <randomuser.RandomUser at 0x7f0230456430>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_users = RandomUser.generate_users(5000, {\"nat\": \"us\"})\n",
    "print(len(my_users))\n",
    "my_users[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fc4ca61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '895-76-0473',\n",
       "  'first_name': 'Christopher',\n",
       "  'last_name': 'Morgan',\n",
       "  'state': 'Nebraska',\n",
       "  'zip': 73093,\n",
       "  'lat_long': {'latitude': '6.4442', 'longitude': '-78.5063'}},\n",
       " {'user_id': '218-53-2453',\n",
       "  'first_name': 'Riley',\n",
       "  'last_name': 'Franklin',\n",
       "  'state': 'Indiana',\n",
       "  'zip': 43053,\n",
       "  'lat_long': {'latitude': '86.3108', 'longitude': '81.9157'}},\n",
       " {'user_id': '290-41-4495',\n",
       "  'first_name': 'Tammy',\n",
       "  'last_name': 'Gutierrez',\n",
       "  'state': 'Delaware',\n",
       "  'zip': 11862,\n",
       "  'lat_long': {'latitude': '10.2281', 'longitude': '-141.5519'}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of 10 random users\n",
    "\n",
    "user_dicts = list(map(get_user_info, my_users))\n",
    "user_dicts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2211b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the users to a json file such that each line is a separate, self-contained valid JSON object.\n",
    "# This is a special JSON format callednewline-delimited JSON.\n",
    "\n",
    "\n",
    "import json\n",
    "f = open(\"data/random_user_dicts.json\", \"w\")\n",
    "for o in user_dicts:\n",
    "    o[\"lat_long\"][\"latitude\"] = float(o[\"lat_long\"][\"latitude\"])\n",
    "    o[\"lat_long\"][\"longitude\"] = float(o[\"lat_long\"][\"longitude\"])\n",
    "\n",
    "    json.dump(o, f)\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96121a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of objects in my RDD is: 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'user_id': '214-08-0103',\n",
       "  'first_name': 'Joe',\n",
       "  'last_name': 'Black',\n",
       "  'state': 'Delaware',\n",
       "  'zip': 57903,\n",
       "  'lat_long': {'latitude': 62.858, 'longitude': 109.5403}},\n",
       " {'user_id': '327-55-8527',\n",
       "  'first_name': 'Hailey',\n",
       "  'last_name': 'Shaw',\n",
       "  'state': 'New Mexico',\n",
       "  'zip': 82415,\n",
       "  'lat_long': {'latitude': 25.672, 'longitude': 62.809}},\n",
       " {'user_id': '381-98-6816',\n",
       "  'first_name': 'Walter',\n",
       "  'last_name': 'Perez',\n",
       "  'state': 'Ohio',\n",
       "  'zip': 99746,\n",
       "  'lat_long': {'latitude': 88.8267, 'longitude': 15.5566}}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_rdd = sc.parallelize(user_dicts)\n",
    "users_rdd_size  = users_rdd.count()\n",
    "print(f\"The number of objects in my RDD is: {users_rdd_size}\")\n",
    "users_rdd.takeSample(False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628f1734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_users_rdd = users_rdd.filter(lambda x: x['state'] in [\"Nebraska\", \"Hawaii\", \"Idaho\"])\n",
    "select_users_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dbd10d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '816-75-8167',\n",
       "  'first_name': 'Mason',\n",
       "  'last_name': 'Patterson',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 64947},\n",
       " {'user_id': '994-86-8073',\n",
       "  'first_name': 'Ida',\n",
       "  'last_name': 'Carpenter',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 86327},\n",
       " {'user_id': '380-06-4689',\n",
       "  'first_name': 'Joe',\n",
       "  'last_name': 'Ray',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 42562},\n",
       " {'user_id': '160-51-2868',\n",
       "  'first_name': 'Nicole',\n",
       "  'last_name': 'Hill',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 73687},\n",
       " {'user_id': '990-61-6010',\n",
       "  'first_name': 'Avery',\n",
       "  'last_name': 'Webb',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 53372},\n",
       " {'user_id': '842-56-6565',\n",
       "  'first_name': 'Carrie',\n",
       "  'last_name': 'Rhodes',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 18556},\n",
       " {'user_id': '500-92-3559',\n",
       "  'first_name': 'Alfred',\n",
       "  'last_name': 'Hoffman',\n",
       "  'state': 'Nebraska',\n",
       "  'zip': 77520},\n",
       " {'user_id': '146-32-0395',\n",
       "  'first_name': 'Dan',\n",
       "  'last_name': 'Long',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 28104},\n",
       " {'user_id': '814-67-3737',\n",
       "  'first_name': 'Roberta',\n",
       "  'last_name': 'Ryan',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 73914},\n",
       " {'user_id': '370-67-6529',\n",
       "  'first_name': 'Tommy',\n",
       "  'last_name': 'Gordon',\n",
       "  'state': 'Nebraska',\n",
       "  'zip': 67816},\n",
       " {'user_id': '084-08-8905',\n",
       "  'first_name': 'Max',\n",
       "  'last_name': 'Bennett',\n",
       "  'state': 'Nebraska',\n",
       "  'zip': 54117}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collec the result means grab them from all the chunk nodes\n",
    "select_users_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef4a3e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building an RDD from a text file.\n",
    "text = sc.textFile('data/pride_and_prejudice.txt', minPartitions=4)\n",
    "### Number of items in the RDD\n",
    "text.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f176316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbe of objects in the RDD is 14579\n",
      "numbe of lines in the text file is 14579\n"
     ]
    }
   ],
   "source": [
    "text_rdd_size = text.count()\n",
    "print(f\"numbe of objects in the RDD is {text_rdd_size}\")\n",
    "\n",
    "nb_lines = len(open(\"data/pride_and_prejudice.txt\").readlines())\n",
    "print(f\"numbe of lines in the text file is {nb_lines}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81655fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of subset_x is: 10\n",
      "\n",
      "type of subset_x is: <class 'list'>\n",
      "\n",
      "subset_x is:\n",
      "['The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen', '', 'This eBook is for the use of anyone anywhere in the United States and', 'most other parts of the world at no cost and with almost no restrictions', 'whatsoever. You may copy it, give it away or re-use it under the terms', 'of the Project Gutenberg License included with this eBook or online at', 'www.gutenberg.org. If you are not located in the United States, you', 'will have to check the laws of the country where you are located before', 'using this eBook.', '']\n"
     ]
    }
   ],
   "source": [
    "subset_x = text.take(10)\n",
    "print(f\"len of subset_x is: {len(subset_x)}\\n\")\n",
    "print(f\"type of subset_x is: {type(subset_x)}\\n\")\n",
    "print(f\"subset_x is:\\n{subset_x}\")\n",
    "\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "883d4b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['THE',\n",
       "  'PROJECT',\n",
       "  'GUTENBERG',\n",
       "  'EBOOK',\n",
       "  'OF',\n",
       "  'PRIDE',\n",
       "  'AND',\n",
       "  'PREJUDICE',\n",
       "  'BY',\n",
       "  'JANE',\n",
       "  'AUSTEN'],\n",
       " [],\n",
       " ['THIS',\n",
       "  'EBOOK',\n",
       "  'IS',\n",
       "  'FOR',\n",
       "  'THE',\n",
       "  'USE',\n",
       "  'OF',\n",
       "  'ANYONE',\n",
       "  'ANYWHERE',\n",
       "  'IN',\n",
       "  'THE',\n",
       "  'UNITED',\n",
       "  'STATES',\n",
       "  'AND'],\n",
       " ['MOST',\n",
       "  'OTHER',\n",
       "  'PARTS',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'WORLD',\n",
       "  'AT',\n",
       "  'NO',\n",
       "  'COST',\n",
       "  'AND',\n",
       "  'WITH',\n",
       "  'ALMOST',\n",
       "  'NO',\n",
       "  'RESTRICTIONS'],\n",
       " ['WHATSOEVER',\n",
       "  'YOU',\n",
       "  'MAY',\n",
       "  'COPY',\n",
       "  'IT',\n",
       "  'GIVE',\n",
       "  'IT',\n",
       "  'AWAY',\n",
       "  'OR',\n",
       "  'RE',\n",
       "  'USE',\n",
       "  'IT',\n",
       "  'UNDER',\n",
       "  'THE',\n",
       "  'TERMS'],\n",
       " ['OF',\n",
       "  'THE',\n",
       "  'PROJECT',\n",
       "  'GUTENBERG',\n",
       "  'LICENSE',\n",
       "  'INCLUDED',\n",
       "  'WITH',\n",
       "  'THIS',\n",
       "  'EBOOK',\n",
       "  'OR',\n",
       "  'ONLINE',\n",
       "  'AT'],\n",
       " ['WWW',\n",
       "  'GUTENBERG',\n",
       "  'ORG',\n",
       "  'IF',\n",
       "  'YOU',\n",
       "  'ARE',\n",
       "  'NOT',\n",
       "  'LOCATED',\n",
       "  'IN',\n",
       "  'THE',\n",
       "  'UNITED',\n",
       "  'STATES',\n",
       "  'YOU'],\n",
       " ['WILL',\n",
       "  'HAVE',\n",
       "  'TO',\n",
       "  'CHECK',\n",
       "  'THE',\n",
       "  'LAWS',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'COUNTRY',\n",
       "  'WHERE',\n",
       "  'YOU',\n",
       "  'ARE',\n",
       "  'LOCATED',\n",
       "  'BEFORE'],\n",
       " ['USING', 'THIS', 'EBOOK'],\n",
       " [],\n",
       " ['TITLE', 'PRIDE', 'AND', 'PREJUDICE'],\n",
       " [],\n",
       " ['AUTHOR', 'JANE', 'AUSTEN'],\n",
       " [],\n",
       " ['RELEASE', 'DATE', 'JUNE', 'EBOOK'],\n",
       " ['MOST', 'RECENTLY', 'UPDATED', 'AUGUST'],\n",
       " [],\n",
       " ['LANGUAGE', 'ENGLISH'],\n",
       " [],\n",
       " ['CHARACTER', 'SET', 'ENCODING', 'UTF'],\n",
       " [],\n",
       " ['PRODUCED', 'BY', 'ANONYMOUS', 'VOLUNTEERS', 'AND', 'DAVID', 'WIDGER'],\n",
       " [],\n",
       " ['START',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'PROJECT',\n",
       "  'GUTENBERG',\n",
       "  'EBOOK',\n",
       "  'PRIDE',\n",
       "  'AND',\n",
       "  'PREJUDICE'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['THERE',\n",
       "  'IS',\n",
       "  'AN',\n",
       "  'ILLUSTRATED',\n",
       "  'EDITION',\n",
       "  'OF',\n",
       "  'THIS',\n",
       "  'TITLE',\n",
       "  'WHICH',\n",
       "  'MAY',\n",
       "  'VIEWED',\n",
       "  'AT',\n",
       "  'EBOOK'],\n",
       " [],\n",
       " [],\n",
       " ['COVER'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['PRIDE', 'AND', 'PREJUDICE'],\n",
       " [],\n",
       " ['BY', 'JANE', 'AUSTEN'],\n",
       " [],\n",
       " ['CONTENTS'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " [],\n",
       " ['CHAPTER'],\n",
       " []]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.map(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7b2eba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'EBOOK',\n",
       " 'OF',\n",
       " 'PRIDE',\n",
       " 'AND',\n",
       " 'PREJUDICE',\n",
       " 'BY',\n",
       " 'JANE',\n",
       " 'AUSTEN',\n",
       " 'THIS',\n",
       " 'EBOOK',\n",
       " 'IS',\n",
       " 'FOR',\n",
       " 'THE',\n",
       " 'USE',\n",
       " 'OF',\n",
       " 'ANYONE',\n",
       " 'ANYWHERE',\n",
       " 'IN',\n",
       " 'THE',\n",
       " 'UNITED',\n",
       " 'STATES',\n",
       " 'AND',\n",
       " 'MOST',\n",
       " 'OTHER',\n",
       " 'PARTS',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'WORLD',\n",
       " 'AT',\n",
       " 'NO',\n",
       " 'COST',\n",
       " 'AND',\n",
       " 'WITH',\n",
       " 'ALMOST',\n",
       " 'NO',\n",
       " 'RESTRICTIONS',\n",
       " 'WHATSOEVER',\n",
       " 'YOU',\n",
       " 'MAY',\n",
       " 'COPY',\n",
       " 'IT',\n",
       " 'GIVE',\n",
       " 'IT',\n",
       " 'AWAY',\n",
       " 'OR',\n",
       " 'RE',\n",
       " 'USE',\n",
       " 'IT',\n",
       " 'UNDER',\n",
       " 'THE',\n",
       " 'TERMS',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'LICENSE',\n",
       " 'INCLUDED']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.flatMap(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6601d40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126018"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19ce8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THE', 1),\n",
       " ('PROJECT', 1),\n",
       " ('GUTENBERG', 1),\n",
       " ('EBOOK', 1),\n",
       " ('OF', 1),\n",
       " ('PRIDE', 1),\n",
       " ('AND', 1),\n",
       " ('PREJUDICE', 1),\n",
       " ('BY', 1),\n",
       " ('JANE', 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to do something like the following\n",
    "# words_mapped = words.map(lambda x: (x,1))\n",
    "\n",
    "words_mapped = words.map(lambda x: (x,1))\n",
    "words_mapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d160702f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[32] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_map = words_mapped.sortByKey()\n",
    "sorted_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d0107be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTTLE', 1),\n",
       " ('BOTTLE', 1),\n",
       " ('BOTTOM', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUND', 1),\n",
       " ('BOUND', 1),\n",
       " ('BOUND', 1),\n",
       " ('BOUNDARY', 1),\n",
       " ('BOUNDARY', 1),\n",
       " ('BOUNDLESS', 1),\n",
       " ('BOUNDS', 1),\n",
       " ('BOUNTY', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_map.take(20000)[18000:18080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62131881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRIDE', 52),\n",
       " ('UNITED', 22),\n",
       " ('OTHER', 227),\n",
       " ('WORLD', 68),\n",
       " ('NO', 501),\n",
       " ('GIVE', 127),\n",
       " ('LICENSE', 18),\n",
       " ('WWW', 9),\n",
       " ('ARE', 361),\n",
       " ('TO', 4245),\n",
       " ('DATE', 5),\n",
       " ('UPDATED', 2),\n",
       " ('ENGLISH', 1),\n",
       " ('CHARACTER', 65),\n",
       " ('PRODUCED', 13),\n",
       " ('ILLUSTRATED', 1),\n",
       " ('THAT', 1555),\n",
       " ('POSSESSION', 10),\n",
       " ('LITTLE', 187),\n",
       " ('KNOWN', 58),\n",
       " ('VIEWS', 11),\n",
       " ('CONSIDERED', 23),\n",
       " ('AS', 1193),\n",
       " ('ONE', 273),\n",
       " ('THEIR', 439),\n",
       " ('MR', 784),\n",
       " ('JUST', 72),\n",
       " ('TOLD', 69),\n",
       " ('ME', 427),\n",
       " ('ANSWER', 65),\n",
       " ('WHO', 288),\n",
       " ('TELL', 71),\n",
       " ('HEARING', 24),\n",
       " ('ENOUGH', 106),\n",
       " ('WHY', 53),\n",
       " ('YOUNG', 130),\n",
       " ('MONDAY', 8),\n",
       " ('FOUR', 35),\n",
       " ('MUCH', 327),\n",
       " ('AGREED', 13),\n",
       " ('MICHAELMAS', 2),\n",
       " ('SERVANTS', 13),\n",
       " ('WEEK', 29),\n",
       " ('NAME', 34),\n",
       " ('BINGLEY', 307),\n",
       " ('OH', 96),\n",
       " ('FIVE', 32),\n",
       " ('YEAR', 29),\n",
       " ('FINE', 31),\n",
       " ('CAN', 223),\n",
       " ('NONSENSE', 8),\n",
       " ('THEREFORE', 75),\n",
       " ('VISIT', 53),\n",
       " ('PERHAPS', 76),\n",
       " ('PARTY', 58),\n",
       " ('THAN', 285),\n",
       " ('CONSIDER', 33),\n",
       " ('YOUR', 446),\n",
       " ('ESTABLISHMENT', 6),\n",
       " ('WILLIAM', 46),\n",
       " ('LUCAS', 70),\n",
       " ('DETERMINED', 32),\n",
       " ('THEY', 599),\n",
       " ('FEW', 72),\n",
       " ('HEARTY', 2),\n",
       " ('CONSENT', 13),\n",
       " ('DESIRE', 23),\n",
       " ('BIT', 3),\n",
       " ('OTHERS', 55),\n",
       " ('ALWAYS', 119),\n",
       " ('RECOMMEND', 12),\n",
       " ('SISTERS', 76),\n",
       " ('CHILDREN', 25),\n",
       " ('VEXING', 1),\n",
       " ('POOR', 38),\n",
       " ('NERVES', 4),\n",
       " ('HIGH', 17),\n",
       " ('OLD', 15),\n",
       " ('YEARS', 36),\n",
       " ('SINCE', 59),\n",
       " ('CAPRICE', 4),\n",
       " ('INSUFFICIENT', 10),\n",
       " ('UNDERSTANDING', 21),\n",
       " ('FANCIED', 7),\n",
       " ('NERVOUS', 4),\n",
       " ('VISITING', 5),\n",
       " ('WAITED', 7),\n",
       " ('ASSURING', 4),\n",
       " ('TILL', 93),\n",
       " ('EVENING', 74),\n",
       " ('AFTER', 200),\n",
       " ('FOLLOWING', 27),\n",
       " ('TRIMMING', 1),\n",
       " ('SUDDENLY', 16),\n",
       " ('ADDRESSED', 17),\n",
       " ('WE', 255),\n",
       " ('FORGET', 17),\n",
       " ('SHALL', 164),\n",
       " ('PROMISED', 21),\n",
       " ('TWO', 131),\n",
       " ('NIECES', 10),\n",
       " ('UNABLE', 14),\n",
       " ('SCOLDING', 2),\n",
       " ('KITTY', 71),\n",
       " ('DISCRETION', 5),\n",
       " ('FRETFULLY', 2),\n",
       " ('BALL', 36),\n",
       " ('FRIEND', 112),\n",
       " ('TEASING', 7),\n",
       " ('ACT', 9),\n",
       " ('STARED', 5),\n",
       " ('MEANING', 14),\n",
       " ('EXCLAMATION', 2),\n",
       " ('INTRODUCTION', 11),\n",
       " ('LAID', 8),\n",
       " ('_THERE_', 5),\n",
       " ('REFLECTION', 10),\n",
       " ('EXTRACTS', 2),\n",
       " ('WISHED', 39),\n",
       " ('HEAR', 71),\n",
       " ('UNLUCKY', 10),\n",
       " ('ACTUALLY', 12),\n",
       " ('ASTONISHMENT', 30),\n",
       " ('TOO', 141),\n",
       " ('NEGLECT', 11),\n",
       " ('JOKE', 3),\n",
       " ('NEVER', 218),\n",
       " ('CHOOSE', 20),\n",
       " ('LEFT', 65),\n",
       " ('AMENDS', 5),\n",
       " ('MATTER', 29),\n",
       " ('MAKING', 36),\n",
       " ('EVERY', 198),\n",
       " ('YOUNGEST', 13),\n",
       " ('CONJECTURING', 2),\n",
       " ('DETERMINING', 2),\n",
       " ('COULD', 526),\n",
       " ('SUFFICIENT', 4),\n",
       " ('DRAW', 6),\n",
       " ('SATISFACTORY', 7),\n",
       " ('BAREFACED', 1),\n",
       " ('QUESTIONS', 16),\n",
       " ('ELUDED', 1),\n",
       " ('WERE', 565),\n",
       " ('HAND', 25),\n",
       " ('HIGHLY', 28),\n",
       " ('EXTREMELY', 15),\n",
       " ('AGREEABLE', 45),\n",
       " ('ASSEMBLY', 9),\n",
       " ('FOND', 16),\n",
       " ('DANCING', 21),\n",
       " ('CERTAIN', 57),\n",
       " ('TOWARDS', 68),\n",
       " ('HOPES', 23),\n",
       " ('EQUALLY', 20),\n",
       " ('WISH', 97),\n",
       " ('LIBRARY', 24),\n",
       " ('BEING', 175),\n",
       " ('SIGHT', 17),\n",
       " ('WHOSE', 59),\n",
       " ('RODE', 2),\n",
       " ('BLACK', 1),\n",
       " ('HORSE', 3),\n",
       " ('AFTERWARDS', 32),\n",
       " ('DISPATCHED', 4),\n",
       " ('COURSES', 3),\n",
       " ('IMAGINE', 27),\n",
       " ('FEARS', 9),\n",
       " ('SEVEN', 3),\n",
       " ('GENTLEMEN', 38),\n",
       " ('INSTEAD', 21),\n",
       " ('COUSIN', 49),\n",
       " ('ALTOGETHER', 8),\n",
       " ('COUNTENANCE', 30),\n",
       " ('EASY', 25),\n",
       " ('WOMEN', 19),\n",
       " ('AIR', 30),\n",
       " ('BROTHER', 79),\n",
       " ('LOOKED', 75),\n",
       " ('HAVING', 94),\n",
       " ('DECLARED', 15),\n",
       " ('TIDE', 2),\n",
       " ('ESTATE', 20),\n",
       " ('SAVE', 3),\n",
       " ('PRINCIPAL', 14),\n",
       " ('UNRESERVED', 3),\n",
       " ('ANGRY', 17),\n",
       " ('WALKING', 26),\n",
       " ('EVERYBODY', 38),\n",
       " ('VIOLENT', 9),\n",
       " ('DISLIKE', 22),\n",
       " ('DURING', 27),\n",
       " ('PART', 64),\n",
       " ('CONVERSATION', 61),\n",
       " ('STUPID', 4),\n",
       " ('DETEST', 3),\n",
       " ('INSUPPORTABLE', 3),\n",
       " ('WHOM', 85),\n",
       " ('PUNISHMENT', 5),\n",
       " ('PRETTY', 24),\n",
       " ('BEAUTIFUL', 15),\n",
       " ('ROUND', 17),\n",
       " ('CATCHING', 1),\n",
       " ('TEMPT', 4),\n",
       " ('_ME_', 21),\n",
       " ('WASTING', 1),\n",
       " ('REMAINED', 23),\n",
       " ('GRATIFIED', 14),\n",
       " ('YET', 73),\n",
       " ('SPIRITS', 39),\n",
       " ('FOUND', 68),\n",
       " ('BOOK', 18),\n",
       " ('DEAL', 34),\n",
       " ('RAISED', 6),\n",
       " ('DISAPPOINTED', 12),\n",
       " ('GOT', 25),\n",
       " ('ELEGANT', 14),\n",
       " ('LACE', 1),\n",
       " ('GOWN', 7),\n",
       " ('EXAGGERATION', 1),\n",
       " ('ADDED', 71),\n",
       " ('LOSE', 11),\n",
       " ('SUITING', 1),\n",
       " ('DOWNS', 1),\n",
       " ('FORMER', 38),\n",
       " ('PERFECT', 17),\n",
       " ('BREEDING', 6),\n",
       " ('POSSIBLY', 8),\n",
       " ('THEREBY', 2),\n",
       " ('SEEING', 60),\n",
       " ('THANKS', 14),\n",
       " ('GALLANTRY', 8),\n",
       " ('LEAVE', 62),\n",
       " ('CENSURING', 1),\n",
       " ('MAKES', 8),\n",
       " ('SENSE', 36),\n",
       " ('MEETS', 2),\n",
       " ('BELONGS', 2),\n",
       " ('PLEASE', 17),\n",
       " ('POWER', 42),\n",
       " ('PRIVATE', 11),\n",
       " ('SEMINARIES', 1),\n",
       " ('POUNDS', 24),\n",
       " ('HABIT', 7),\n",
       " ('DEEPLY', 5),\n",
       " ('TRADE', 4),\n",
       " ('SOMETIMES', 35),\n",
       " ('PROVIDED', 11),\n",
       " ('WHETHER', 54),\n",
       " ('ANXIOUS', 26),\n",
       " ('SITUATION', 38),\n",
       " ('FRIENDSHIP', 9),\n",
       " ('DUCTILITY', 1),\n",
       " ('GREATER', 25),\n",
       " ('HIGHEST', 9),\n",
       " ('SAME', 70),\n",
       " ('HAUGHTY', 3),\n",
       " ('CONTINUALLY', 6),\n",
       " ('MERYTON', 57),\n",
       " ('CHARACTERISTIC', 2),\n",
       " ('PRETTIER', 2),\n",
       " ('SMALLEST', 11),\n",
       " ('RECEIVED', 66),\n",
       " ('KNIGHTHOOD', 2),\n",
       " ('MARKET', 2),\n",
       " ('PERIOD', 15),\n",
       " ('SOLELY', 4),\n",
       " ('ELATED', 1),\n",
       " ('NATURE', 24),\n",
       " ('ST', 7),\n",
       " ('INTELLIGENT', 4),\n",
       " ('NECESSARY', 31),\n",
       " ('YES', 76),\n",
       " ('HARDLY', 46),\n",
       " ('QUESTION', 23),\n",
       " ('OPINIONS', 8),\n",
       " ('LISTENING', 10),\n",
       " ('_TOLERABLE_', 1),\n",
       " ('HEAD', 32),\n",
       " ('NIGHT', 35),\n",
       " ('OPENING', 13),\n",
       " ('LIPS', 9),\n",
       " ('EAT', 5),\n",
       " ('OFFEND', 3),\n",
       " ('EVERYTHING', 45),\n",
       " ('EXPRESS', 25),\n",
       " ('FORGIVE', 10),\n",
       " ('OBSERVED', 27),\n",
       " ('SCORE', 4),\n",
       " ('VANITY', 18),\n",
       " ('WINE', 3),\n",
       " ('GREW', 6),\n",
       " ('INTOLERABLE', 3),\n",
       " ('GREATEST', 20),\n",
       " ('SUPERCILIOUSNESS', 1),\n",
       " ('PROBABILITY', 9),\n",
       " ('COMPOSURE', 16),\n",
       " ('SUSPICIONS', 4),\n",
       " ('PUBLIC', 15),\n",
       " ('FIXING', 2),\n",
       " ('CONSOLATION', 10),\n",
       " ('DARK', 2),\n",
       " ('ATTACHMENT', 27),\n",
       " ('SAFE', 8),\n",
       " ('ENCOURAGEMENT', 10),\n",
       " ('OUT', 134),\n",
       " ('UNDOUBTEDLY', 10),\n",
       " ('PARTIAL', 10),\n",
       " ('ENDEAVOUR', 20),\n",
       " ('TOLERABLY', 14),\n",
       " ('ACTING', 3),\n",
       " ('REASONABLENESS', 1),\n",
       " ('DINED', 7),\n",
       " ('EVENINGS', 4),\n",
       " ('UNFOLDED', 5),\n",
       " ('SUCCESS', 19),\n",
       " ('MARRIAGE', 66),\n",
       " ('SIMILAR', 5),\n",
       " ('OCCUPIED', 6),\n",
       " ('FAR', 58),\n",
       " ('SCARCELY', 45),\n",
       " ('SOONER', 13),\n",
       " ('RENDERED', 11),\n",
       " ('FAILURE', 1),\n",
       " ('ACKNOWLEDGE', 8),\n",
       " ('ASSERTING', 1),\n",
       " ('CAUGHT', 11),\n",
       " ('APPROACHING', 12),\n",
       " ('DEFIED', 1),\n",
       " ('PROVOKING', 3),\n",
       " ('ENERGETIC', 1),\n",
       " ('TURN', 27),\n",
       " ('TEASED', 2),\n",
       " ('INSTRUMENT', 11),\n",
       " ('WANTING', 7),\n",
       " ('GRAVELY', 4),\n",
       " ('BREATH', 2),\n",
       " ('PORRIDGE', 1),\n",
       " ('MINE', 10),\n",
       " ('WORKED', 4),\n",
       " ('ACCOMPLISHMENTS', 5),\n",
       " ('GENIUS', 2),\n",
       " ('TASTE', 14),\n",
       " ('HIGHER', 4),\n",
       " ('EXCELLENCE', 2),\n",
       " ('JOINED', 28),\n",
       " ('INDIGNATION', 8),\n",
       " ('MODE', 5),\n",
       " ('POLISHED', 2),\n",
       " ('SOCIETIES', 2),\n",
       " ('SAVAGE', 1),\n",
       " ('PAUSE', 18),\n",
       " ('CONCLUDE', 5),\n",
       " ('SOCIETY', 32),\n",
       " ('INSTANT', 6),\n",
       " ('MOVING', 5),\n",
       " ('RECEIVE', 30),\n",
       " ('DISCOMPOSURE', 3),\n",
       " ('SHAKE', 2),\n",
       " ('ATTEMPT', 22),\n",
       " ('SMILING', 10),\n",
       " ('INDUCEMENT', 5),\n",
       " ('ARCHLY', 2),\n",
       " ('ACCOSTED', 2),\n",
       " ('INSIPIDITY', 1),\n",
       " ('TOTALLY', 8),\n",
       " ('MEDITATING', 3),\n",
       " ('INTREPIDITY', 1),\n",
       " ('FAVOURITE', 22),\n",
       " ('PRAY', 26),\n",
       " ('JUMPS', 1),\n",
       " ('INDIFFERENCE', 17),\n",
       " ('UNFORTUNATELY', 3),\n",
       " ('MALE', 1),\n",
       " ('RELATION', 11),\n",
       " ('DISTANCE', 16),\n",
       " ('OFFERED', 9),\n",
       " ('AMUSE', 5),\n",
       " ('LEARN', 11),\n",
       " ('REMAIN', 11),\n",
       " ('LENGTH', 32),\n",
       " ('ANIMATION', 4),\n",
       " ('ENSIGN', 1),\n",
       " ('SILLIEST', 1),\n",
       " ('SMART', 2),\n",
       " ('WATSON', 1),\n",
       " ('NOTE', 8),\n",
       " ('SERVANT', 18),\n",
       " ('SPARKLED', 2),\n",
       " ('HASTE', 9),\n",
       " ('DINE', 16),\n",
       " ('LOUISA', 6),\n",
       " ('DANGER', 19),\n",
       " ('RECEIPT', 4),\n",
       " ('STAY', 38),\n",
       " ('HORSES', 8),\n",
       " ('WANTED', 24),\n",
       " ('ANSWERED', 34),\n",
       " ('RAINED', 1),\n",
       " ('UNEASY', 8),\n",
       " ('INTERMISSION', 3),\n",
       " ('AWARE', 14),\n",
       " ('GETTING', 7),\n",
       " ('YESTERDAY', 13),\n",
       " ('ILLNESS', 2),\n",
       " ('ORDERS', 7),\n",
       " ('DYING', 3),\n",
       " ('HORSEWOMAN', 1),\n",
       " ('DIRT', 2),\n",
       " ('IMPULSE', 3),\n",
       " ('REPAIRED', 4),\n",
       " ('WIVES', 1),\n",
       " ('PACE', 1),\n",
       " ('SPRINGING', 2),\n",
       " ('FINDING', 13),\n",
       " ('STOCKINGS', 1),\n",
       " ('APPEARANCE', 30),\n",
       " ('INCREDIBLE', 3),\n",
       " ('COMING', 60),\n",
       " ('LATTER', 22),\n",
       " ('FEVERISH', 2),\n",
       " ('ALARM', 14),\n",
       " ('APOTHECARY', 2),\n",
       " ('SUPPOSED', 26),\n",
       " ('READILY', 17),\n",
       " ('SYMPTOMS', 2),\n",
       " ('ACUTELY', 4),\n",
       " ('QUIT', 10),\n",
       " ('CLOCK', 6),\n",
       " ('THANKFULLY', 1),\n",
       " ('CONSENTED', 2),\n",
       " ('O', 5),\n",
       " ('DISLIKED', 4),\n",
       " ('BELIEVED', 38),\n",
       " ('INDOLENT', 1),\n",
       " ('WILD', 11),\n",
       " ('NONSENSICAL', 3),\n",
       " ('SCAMPERING', 1),\n",
       " ('BLOWSY', 1),\n",
       " ('INCHES', 1),\n",
       " ('PICTURE', 13),\n",
       " ('INCLINED', 10),\n",
       " ('WHATEVER', 34),\n",
       " ('ABOMINABLE', 6),\n",
       " ('DECORUM', 4),\n",
       " ('LAUGHED', 11),\n",
       " ('UNCLES', 3),\n",
       " ('FILL', 1),\n",
       " ('MATERIALLY', 5),\n",
       " ('INDULGED', 2),\n",
       " ('TENDERNESS', 3),\n",
       " ('LEAVING', 18),\n",
       " ('LATE', 19),\n",
       " ('BELOW', 8),\n",
       " ('READING', 15),\n",
       " ('CENSURE', 5),\n",
       " ('THANKED', 7),\n",
       " ('LYING', 1),\n",
       " ('BUYING', 1),\n",
       " ('WHOLLY', 26),\n",
       " ('HEIGHT', 3),\n",
       " ('PIANOFORTE', 5),\n",
       " ('EXQUISITE', 1),\n",
       " ('TABLES', 6),\n",
       " ('LIST', 3),\n",
       " ('OTHERWISE', 16),\n",
       " ('AGREEING', 3),\n",
       " ('DOZEN', 2),\n",
       " ('FAITHFUL', 3),\n",
       " ('MODERN', 2),\n",
       " ('LANGUAGES', 1),\n",
       " ('POSSESS', 5),\n",
       " ('LONGER', 44),\n",
       " ('_ONLY_', 1),\n",
       " ('_ANY_', 1),\n",
       " ('DESCRIBE', 7),\n",
       " ('UNDERVALUING', 1),\n",
       " ('PALTRY', 2),\n",
       " ('ART', 3),\n",
       " ('MEANNESS', 1),\n",
       " ('ARTS', 4),\n",
       " ('CAPTIVATION', 1),\n",
       " ('BEARS', 2),\n",
       " ('DESPICABLE', 1),\n",
       " ('WORSE', 13),\n",
       " ('SENT', 21),\n",
       " ('MISERABLE', 6),\n",
       " ('HOUSEKEEPER', 16),\n",
       " ('HEALTH', 18),\n",
       " ('ADVISABLE', 6),\n",
       " ('REMOVAL', 7),\n",
       " ('PROFUSE', 1),\n",
       " ('PROSPECT', 14),\n",
       " ('GRAVEL', 3),\n",
       " ('HURRY', 11),\n",
       " ('DONE', 92),\n",
       " ('SUFFERED', 8),\n",
       " ('STUDIER', 1),\n",
       " ('STUDY', 7),\n",
       " ('_MOST_', 1),\n",
       " ('SUBJECTS', 10),\n",
       " ('CONFINED', 5),\n",
       " ('UNVARYING', 1),\n",
       " ('SHOPS', 1),\n",
       " ('SMILE', 29),\n",
       " ('PERSONS', 9),\n",
       " ('MOUTHS', 2),\n",
       " ('DIFFERENTLY', 7),\n",
       " ('JUDGE', 12),\n",
       " ('_VERY_', 1),\n",
       " ('FIFTEEN', 4),\n",
       " ('TREMBLE', 1),\n",
       " ('TROUBLING', 1),\n",
       " ('GRACIOUSNESS', 1),\n",
       " ('ORDERED', 8),\n",
       " ('RESULT', 5),\n",
       " ('ANIMAL', 1),\n",
       " ('DINNERS', 2),\n",
       " ('ATTACK', 4),\n",
       " ('EAR', 1),\n",
       " ('SHAME', 10),\n",
       " ('PREVAILED', 15),\n",
       " ('SLOWLY', 4),\n",
       " ('WRITING', 16),\n",
       " ('SEATED', 15),\n",
       " ('WATCHING', 8),\n",
       " ('PIQUET', 1),\n",
       " ('COMMENDATIONS', 1),\n",
       " ('HANDWRITING', 1),\n",
       " ('UNCONCERN', 3),\n",
       " ('FORMED', 16),\n",
       " ('CURIOUS', 5),\n",
       " ('UNISON', 1),\n",
       " ('PENS', 1),\n",
       " ('INFINITELY', 4),\n",
       " ('DEFER', 3),\n",
       " ('JUSTICE', 15),\n",
       " ('SYLLABLES', 1),\n",
       " ('WRITES', 1),\n",
       " ('CARELESS', 2),\n",
       " ('BLOTS', 1),\n",
       " ('RAPIDLY', 2),\n",
       " ('HUMILITY', 5),\n",
       " ('REPROOF', 4),\n",
       " ('PIECE', 6),\n",
       " ('PROCEEDING', 6),\n",
       " ('POSSESSOR', 2),\n",
       " ('IMPERFECTION', 1),\n",
       " ('CELERITY', 1),\n",
       " ('ADHERING', 1),\n",
       " ('EXPLAIN', 6),\n",
       " ('ALLOWING', 8),\n",
       " ('OFFERING', 3),\n",
       " ('_PERSUASION_', 1),\n",
       " ('REQUESTER', 1),\n",
       " ('OCCURS', 1),\n",
       " ('DISCUSS', 2),\n",
       " ('CHANGE', 25),\n",
       " ('APPERTAIN', 1),\n",
       " ('WEIGHT', 6),\n",
       " ('OCCASIONS', 8),\n",
       " ('SUNDAY', 9),\n",
       " ('DISPUTES', 1),\n",
       " ('ALACRITY', 5),\n",
       " ('POLITE', 5),\n",
       " ('REPREHENSIBLE', 3),\n",
       " ('SONGS', 1),\n",
       " ('VARIED', 3),\n",
       " ('REEL', 2),\n",
       " ('PREMEDITATED', 1),\n",
       " ('BEWITCHED', 1),\n",
       " ('RECOVERY', 3),\n",
       " ('RID', 5),\n",
       " ('TRIED', 16),\n",
       " ('PROVOKE', 5),\n",
       " ('GUEST', 5),\n",
       " ('ALLIANCE', 3),\n",
       " ('PROFESSION', 7),\n",
       " ('COPIED', 3),\n",
       " ('LAUGHINGLY', 2),\n",
       " ('SPOILT', 2),\n",
       " ('COUPLE', 8),\n",
       " ('POWERS', 6),\n",
       " ('ACCURACY', 1),\n",
       " ('TOWARD', 3),\n",
       " ('STEPS', 5),\n",
       " ('DIFFUSENESS', 1),\n",
       " ('SALUTATION', 4),\n",
       " ('FULL', 32),\n",
       " ('PILING', 1),\n",
       " ('FIREPLACE', 1),\n",
       " ('OPPOSITE', 11),\n",
       " ('TEA', 13),\n",
       " ('OBTAINED', 4),\n",
       " ('JUSTIFY', 6),\n",
       " ('PRINCIPALLY', 5),\n",
       " ('BRACELETS', 1),\n",
       " ('PERPETUALLY', 3),\n",
       " ('EXHAUSTED', 1),\n",
       " ('CHOSEN', 8),\n",
       " ('CAST', 2),\n",
       " ('BEGINS', 4),\n",
       " ('WHITE', 3),\n",
       " ('BALLS', 8),\n",
       " ('USUAL', 34),\n",
       " ('RATIONAL', 10),\n",
       " ('AIMED', 1),\n",
       " ('INTERFERE', 1),\n",
       " ('SUREST', 1),\n",
       " ('EXPLANATION', 10),\n",
       " ('METHOD', 6),\n",
       " ('DEFY', 4),\n",
       " ('HUG', 1),\n",
       " ('LOSS', 17),\n",
       " ('DEARLY', 3),\n",
       " ('WHIMS', 2),\n",
       " ('PRECISELY', 1),\n",
       " ('WEAKNESSES', 1),\n",
       " ('PRESUME', 2),\n",
       " ('PRETENSION', 1),\n",
       " ('VOUCH', 3),\n",
       " ('_LAUGH_', 1),\n",
       " ('TENDENCY', 3),\n",
       " ('EDUCATION', 11),\n",
       " ('STAYING', 10),\n",
       " ('INTRUDING', 2),\n",
       " ('EXCITED', 10),\n",
       " ('PROPOSED', 5),\n",
       " ('JEALOUSY', 4),\n",
       " ('FIRM', 3),\n",
       " ('_NOW_', 6),\n",
       " ('CRUSHING', 1),\n",
       " ('LACONIC', 1),\n",
       " ('CIRCLE', 7),\n",
       " ('ABSENCE', 27),\n",
       " ('BASS', 1),\n",
       " ('PRECEDING', 4),\n",
       " ('FLOGGED', 1),\n",
       " ('RING', 5),\n",
       " ('EXPLAINED', 9),\n",
       " ('PLEASES', 2),\n",
       " ('HARDEST', 1),\n",
       " ('ENTAIL', 8),\n",
       " ('RAIL', 1),\n",
       " ('WESTERHAM', 1),\n",
       " ('OCTOBER_', 1),\n",
       " ('HONOURED', 9),\n",
       " ('PATRONAGE', 4),\n",
       " ('HONOURABLE', 6),\n",
       " ('DE', 39),\n",
       " ('WIDOW', 3),\n",
       " ('BOUNTY', 1),\n",
       " ('PERFORM', 5),\n",
       " ('RITES', 1),\n",
       " ('INSTITUTED', 1),\n",
       " ('PEACE', 7),\n",
       " ('OVERTURES', 1),\n",
       " ('KINDLY', 7),\n",
       " ('HEREAFTER', 3),\n",
       " ('SATISFACTION', 27),\n",
       " ('HOSPITALITY', 3),\n",
       " ('ENNIGHT', 1),\n",
       " ('OBJECTING', 3),\n",
       " ('WISHER', 1),\n",
       " ('PARISHIONERS', 1),\n",
       " ('ODDITY', 1),\n",
       " ('REVERSE', 1),\n",
       " ('SERVILITY', 1),\n",
       " ('PROMISES', 3),\n",
       " ('WEEKS', 18),\n",
       " ('PUNCTUAL', 3),\n",
       " ('HEAVY', 4),\n",
       " ('STATELY', 3),\n",
       " ('FORMAL', 8),\n",
       " ('DESTITUTE', 1),\n",
       " ('FURNITURE', 10),\n",
       " ('FUTURE', 18),\n",
       " ('BEGGED', 8),\n",
       " ('EXCELLENCY', 1),\n",
       " ('OWING', 4),\n",
       " ('SOLEMNITY', 5),\n",
       " ('ASPECT', 6),\n",
       " ('AFFABILITY', 4),\n",
       " ('DISCOURSES', 1),\n",
       " ('_HE_', 11),\n",
       " ('COURT', 4),\n",
       " ('INDIFFERENT', 16),\n",
       " ('BRITISH', 1),\n",
       " ('DUCHESS', 1),\n",
       " ('PECULIARLY', 3),\n",
       " ('BOUND', 3),\n",
       " ('RESOLUTE', 1),\n",
       " ('BID', 1),\n",
       " ('ADVANTAGEOUS', 2),\n",
       " ('RESENT', 4),\n",
       " ('WEAK', 7),\n",
       " ('LIVING', 24),\n",
       " ('RETIREMENT', 2),\n",
       " ('UNEXPECTED', 8),\n",
       " ('SUITABLENESS', 3),\n",
       " ('GENEROUS', 12),\n",
       " ('VARY', 3),\n",
       " ('STRICTEST', 1),\n",
       " ('NOTIONS', 3),\n",
       " ('ALTERATION', 8),\n",
       " ('GRACES', 1),\n",
       " ('FORGOTTEN', 12),\n",
       " ('ATTEND', 5),\n",
       " ('DOINGS', 2),\n",
       " ('FREE', 12),\n",
       " ('NOTHINGS', 1),\n",
       " ('ONES', 6),\n",
       " ('RECALL', 2),\n",
       " ('OFFICER', 8),\n",
       " ('ENTREATED', 2),\n",
       " ('PERMISSION', 12),\n",
       " ('CORROBORATED', 1),\n",
       " ('HAPPENING', 2),\n",
       " ('THROWING', 2),\n",
       " ('LOUDLY', 2),\n",
       " ('FETCHED', 1),\n",
       " ('EXCESS', 2),\n",
       " ('CONTEMPLATION', 4),\n",
       " ('LIEUTENANT', 1),\n",
       " ('OCCUPATION', 1),\n",
       " ('TICKETS', 3),\n",
       " ('ADMIRING', 2),\n",
       " ('POINTEDLY', 1),\n",
       " ('CONNECTION', 20),\n",
       " ('CONVEYED', 2),\n",
       " ('SUITABLE', 3),\n",
       " ('PROPRIETOR', 3),\n",
       " ('GRANDEUR', 4),\n",
       " ('RECEIVING', 21),\n",
       " ('LISTENER', 3),\n",
       " ('RETAIL', 1),\n",
       " ('APPROACH', 8),\n",
       " ('BROAD', 1),\n",
       " ('FACED', 1),\n",
       " ('BREATHING', 1),\n",
       " ('FEMALE', 8),\n",
       " ('DULLEST', 1),\n",
       " ('TOPIC', 1),\n",
       " ('ENGROSSING', 1),\n",
       " ('TALKER', 1),\n",
       " ('WILLING', 6),\n",
       " ('HISTORY', 4),\n",
       " ('CAPABLE', 18),\n",
       " ('ASSERTION', 3),\n",
       " ('ASTONISH', 2),\n",
       " ('DISGUSTED', 3),\n",
       " ('ESTIMATED', 1),\n",
       " ('BREATHED', 1),\n",
       " ('SOUL', 2),\n",
       " ('TENDER', 4),\n",
       " ('SCANDALOUS', 3),\n",
       " ('VERILY', 1),\n",
       " ('ENTER', 12),\n",
       " ('QUARTERS', 2),\n",
       " ('SOLITUDE', 3),\n",
       " ('_MUST_', 3),\n",
       " ('GODFATHER', 1),\n",
       " ('ATTACHED', 12),\n",
       " ('INFORMALITY', 1),\n",
       " ('TREAT', 5),\n",
       " ('ASSERT', 5),\n",
       " ('FORFEITED', 1),\n",
       " ('IMPRUDENCE', 9),\n",
       " ('_TO_', 1),\n",
       " ('HATES', 2),\n",
       " ('_WILL_', 7),\n",
       " ('CRUELLY', 3),\n",
       " ('SON', 22),\n",
       " ('COMPETITION', 3),\n",
       " ('CREATURES', 3),\n",
       " ('DESCENDING', 2),\n",
       " ('MALICIOUS', 2),\n",
       " ('CHILDHOOD', 1),\n",
       " ('NEARER', 5),\n",
       " ('IMPULSES', 1),\n",
       " ('LIBERAL', 5),\n",
       " ('DISGRACE', 11),\n",
       " ('SUPERINTENDS', 1),\n",
       " ('PAUSES', 2),\n",
       " ('TRIALS', 1),\n",
       " ('ABILITIES', 6),\n",
       " ('EQUALS', 1),\n",
       " ('MINDED', 2),\n",
       " ('NECESSITY', 11),\n",
       " ('REGARDING', 1),\n",
       " ('ANNE', 5),\n",
       " ('DERIVES', 1),\n",
       " ('AUTHORITATIVE', 2),\n",
       " ('NEPHEW', 21),\n",
       " ('GRACEFULLY', 1),\n",
       " ('WENT', 67),\n",
       " ('INCESSANTLY', 2),\n",
       " ('CROWDED', 3),\n",
       " ('UNKINDNESS', 1),\n",
       " ('ACCIDENT', 6),\n",
       " ('DECEIVED', 13),\n",
       " ('MISREPRESENTED', 2),\n",
       " ('ALIENATED', 1),\n",
       " ('ACTUAL', 6),\n",
       " ('BEHALF', 4),\n",
       " ('TREATING', 1),\n",
       " ('IMPOSED', 5),\n",
       " ('DISTRESSING', 6),\n",
       " ('RISING', 7),\n",
       " ('HURRYING', 3),\n",
       " ('DEPENDED', 4),\n",
       " ('SCRUPLE', 4),\n",
       " ('DISRESPECT', 2),\n",
       " ('LIVELINESS', 4),\n",
       " ('PERFORCE', 1),\n",
       " ('SOUGHT', 5),\n",
       " ('SHOE', 1),\n",
       " ('SUSPENDED', 2),\n",
       " ('FRIDAY', 1),\n",
       " ('ENDURABLE', 1),\n",
       " ('AROSE', 5),\n",
       " ('ABSOLUTE', 4),\n",
       " ('SIGNIFICANT', 3),\n",
       " ('IMMEDIATE', 14),\n",
       " ('FORBEARANCE', 6),\n",
       " ('INJURY', 1),\n",
       " ('SURMOUNT', 1),\n",
       " ('PROVOKED', 3),\n",
       " ('DWELL', 2),\n",
       " ('TRANSITION', 1),\n",
       " ('ODDITIES', 1),\n",
       " ('DISTRESS', 16),\n",
       " ('FORBID', 4),\n",
       " ('ARRANGED', 2),\n",
       " ('UNSOCIAL', 1),\n",
       " ('AMAZE', 1),\n",
       " ('POSTERITY', 1),\n",
       " ('PROVERB', 1),\n",
       " ('STRIKING', 3),\n",
       " ('RESEMBLANCE', 7),\n",
       " ('RESIST', 4),\n",
       " ('DEEPER', 3),\n",
       " ('BLAMING', 3),\n",
       " ('PERCEIVING', 4),\n",
       " ('BELONG', 3),\n",
       " ('CONGRATULATIONS', 12),\n",
       " ('BEWITCHING', 1),\n",
       " ('BRIGHT', 1),\n",
       " ('UPBRAIDING', 1),\n",
       " ('ALLUSION', 4),\n",
       " ('STRIKE', 5),\n",
       " ('UNAPPEASABLE', 1),\n",
       " ('PUZZLE', 2),\n",
       " ('ANGER', 13),\n",
       " ('INFAMOUS', 4),\n",
       " ('SNEER', 1),\n",
       " ('IGNORANCE', 7),\n",
       " ('MARKED', 16),\n",
       " ('OCCURRENCES', 4),\n",
       " ('PROBITY', 1),\n",
       " ('BELIEVES', 1),\n",
       " ('SINCERITY', 4),\n",
       " ('ASSURANCES', 8),\n",
       " ('DISCOURSE', 7),\n",
       " ('_NEPHEW_', 1),\n",
       " ('ADDRESSING', 3),\n",
       " ('SCOPE', 1),\n",
       " ('LAITY', 1),\n",
       " ('CLERGY', 1),\n",
       " ('DICTATES', 1),\n",
       " ('NEGLECTING', 2),\n",
       " ('HABITUAL', 1),\n",
       " ('ADVANCES', 1),\n",
       " ('WATCHED', 7),\n",
       " ('UNRESTRAINED', 1),\n",
       " ('PLAINLY', 6),\n",
       " ('ANIMATING', 1),\n",
       " ('PROMISING', 4),\n",
       " ('CONSIGN', 1),\n",
       " ('AUDIBLE', 1),\n",
       " ('CHICKEN', 1),\n",
       " ('ENTREATY', 7),\n",
       " ('PREVENT', 17),\n",
       " ('SENSATIONS', 3),\n",
       " ('AGONIES', 1),\n",
       " ('DERISION', 2),\n",
       " ('EXHIBIT', 1),\n",
       " ('PRETENDING', 3),\n",
       " ('OFFENSIVE', 2),\n",
       " ('EXCUSED', 2),\n",
       " ('OWES', 2),\n",
       " ('NATUREDLY', 1),\n",
       " ('ALLUSIONS', 2),\n",
       " ('LANGUOR', 1),\n",
       " ('COMPLIMENTING', 1),\n",
       " ('GUESTS', 1),\n",
       " ('ENJOYING', 4),\n",
       " ('WHITHER', 2),\n",
       " ('PREPARATIONS', 2),\n",
       " ('CARRIAGES', 6),\n",
       " ('GATHERING', 1),\n",
       " ('HASTENING', 2),\n",
       " ('EMBARRASSED', 7),\n",
       " ('ADDS', 3),\n",
       " ('RESPECTED', 3),\n",
       " ('DISSEMBLE', 1),\n",
       " ('LAUGHING', 11),\n",
       " ('THIRDLY', 2),\n",
       " ('JENKINSON', 9),\n",
       " ('_OWN_', 1),\n",
       " ('INEVITABLY', 1),\n",
       " ('EXCITE', 3),\n",
       " ('ESTEEM', 11),\n",
       " ('ANIMATED', 2),\n",
       " ('VIOLENCE', 3),\n",
       " ('CENTS', 1),\n",
       " ('ALTAR', 1),\n",
       " ('DISAPPROVE', 1),\n",
       " ('MANIFOLD', 1),\n",
       " ('ATTRACTIONS', 4),\n",
       " ('PORTION', 3),\n",
       " ('EFFECTS', 3),\n",
       " ('REJECTION', 2),\n",
       " ('SANCTIONED', 2),\n",
       " ('PERSEVERANCE', 3),\n",
       " ('DECEPTION', 2),\n",
       " ('REFUSALS', 1),\n",
       " ('APPLY', 4),\n",
       " ('NEGATIVE', 3),\n",
       " ('INTERVIEW', 3),\n",
       " ('BASHFUL', 1),\n",
       " ('GENUINE', 2),\n",
       " ('HEADSTRONG', 4),\n",
       " ('REJECTING', 2),\n",
       " ('ACCEPTING', 5),\n",
       " ('LIABLE', 3),\n",
       " ('CONTRIBUTE', 2),\n",
       " ('NATURED', 11),\n",
       " ('UPROAR', 1),\n",
       " ('HOPELESS', 7),\n",
       " ('CONCLUSION', 8),\n",
       " ('REGARDED', 3),\n",
       " ('INTERFERING', 1),\n",
       " ('DETERMINATION', 3),\n",
       " ('HURT', 6),\n",
       " ('REGRET', 19),\n",
       " ('ENTREATING', 1),\n",
       " ('UNCONCERNED', 2),\n",
       " ('YORK', 1),\n",
       " ('MAINTAIN', 1),\n",
       " ('WARN', 4),\n",
       " ('PITIED', 3),\n",
       " ('DETAINED', 3),\n",
       " ('RESIGNATION', 2),\n",
       " ('PECULIAR', 5),\n",
       " ('POSITIVE', 3),\n",
       " ('REQUESTING', 2),\n",
       " ('ASSIDUOUS', 1),\n",
       " ('TRANSFERRED', 1),\n",
       " ('ABATEMENT', 1),\n",
       " ('PUTTING', 5),\n",
       " ('GROSVENOR', 2),\n",
       " ('HIGHFLOWN', 1),\n",
       " ('CEASE', 3),\n",
       " ('GETS', 1),\n",
       " ('COMFORTLESS', 1),\n",
       " ('HOTEL', 1),\n",
       " ('CROWD', 1),\n",
       " ('ABOUND', 1),\n",
       " ('FOOTING', 2),\n",
       " ('WANTS', 11),\n",
       " ('TRIES', 1),\n",
       " ('GRAND', 2),\n",
       " ('INGENUITY', 2),\n",
       " ('FOUNDATION', 28),\n",
       " ('SUGGESTION', 1),\n",
       " ('DEPARTURE', 6),\n",
       " ('APPEARANCES', 1),\n",
       " ('DIFFIDENT', 1),\n",
       " ('ACCIDENTALLY', 3),\n",
       " ('SOLICITATION', 2),\n",
       " ('PURE', 2),\n",
       " ('JOYFUL', 2),\n",
       " ('PROSPECTS', 4),\n",
       " ('EXPEDIENT', 2),\n",
       " ('APPREHENSION', 12),\n",
       " ('MAID', 3),\n",
       " ('PRESERVATIVE', 2),\n",
       " ('EVADE', 1),\n",
       " ('JOURNEY', 21),\n",
       " ('AVAIL', 1),\n",
       " ('MEDITATED', 2),\n",
       " ('COMMANDED', 2),\n",
       " ('REGAINED', 1),\n",
       " ('FIRMNESS', 1),\n",
       " ('RECONCILED', 2),\n",
       " ('WORLDLY', 2),\n",
       " ('HUMILIATING', 2),\n",
       " ('PANG', 1),\n",
       " ('ANNOUNCE', 4),\n",
       " ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = words_mapped.reduceByKey(lambda x,y: x+y)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee3eca",
   "metadata": {},
   "source": [
    "Since in functional programming we need to always return new data instead of manipulating the data in-place we can re-write the above cleanly using:\n",
    "    \n",
    "```\n",
    "counts_test_2 = text.flatMap(clean_split_line).map(lambda x: (x,1)).reduceByKey(add)\n",
    "counts_test_2.take(100)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd67ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 ms, sys: 14.3 ms, total: 29.3 ms\n",
      "Wall time: 471 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PRIDE', 52),\n",
       " ('UNITED', 22),\n",
       " ('OTHER', 227),\n",
       " ('WORLD', 68),\n",
       " ('NO', 501),\n",
       " ('GIVE', 127),\n",
       " ('LICENSE', 18),\n",
       " ('WWW', 9),\n",
       " ('ARE', 361),\n",
       " ('TO', 4245),\n",
       " ('DATE', 5),\n",
       " ('UPDATED', 2),\n",
       " ('ENGLISH', 1),\n",
       " ('CHARACTER', 65),\n",
       " ('PRODUCED', 13),\n",
       " ('ILLUSTRATED', 1),\n",
       " ('THAT', 1555),\n",
       " ('POSSESSION', 10),\n",
       " ('LITTLE', 187),\n",
       " ('KNOWN', 58),\n",
       " ('VIEWS', 11),\n",
       " ('CONSIDERED', 23),\n",
       " ('AS', 1193),\n",
       " ('ONE', 273),\n",
       " ('THEIR', 439),\n",
       " ('MR', 784),\n",
       " ('JUST', 72),\n",
       " ('TOLD', 69),\n",
       " ('ME', 427),\n",
       " ('ANSWER', 65),\n",
       " ('WHO', 288),\n",
       " ('TELL', 71),\n",
       " ('HEARING', 24),\n",
       " ('ENOUGH', 106),\n",
       " ('WHY', 53),\n",
       " ('YOUNG', 130),\n",
       " ('MONDAY', 8),\n",
       " ('FOUR', 35),\n",
       " ('MUCH', 327),\n",
       " ('AGREED', 13),\n",
       " ('MICHAELMAS', 2),\n",
       " ('SERVANTS', 13),\n",
       " ('WEEK', 29),\n",
       " ('NAME', 34),\n",
       " ('BINGLEY', 307),\n",
       " ('OH', 96),\n",
       " ('FIVE', 32),\n",
       " ('YEAR', 29),\n",
       " ('FINE', 31),\n",
       " ('CAN', 223),\n",
       " ('NONSENSE', 8),\n",
       " ('THEREFORE', 75),\n",
       " ('VISIT', 53),\n",
       " ('PERHAPS', 76),\n",
       " ('PARTY', 58),\n",
       " ('THAN', 285),\n",
       " ('CONSIDER', 33),\n",
       " ('YOUR', 446),\n",
       " ('ESTABLISHMENT', 6),\n",
       " ('WILLIAM', 46),\n",
       " ('LUCAS', 70),\n",
       " ('DETERMINED', 32),\n",
       " ('THEY', 599),\n",
       " ('FEW', 72),\n",
       " ('HEARTY', 2),\n",
       " ('CONSENT', 13),\n",
       " ('DESIRE', 23),\n",
       " ('BIT', 3),\n",
       " ('OTHERS', 55),\n",
       " ('ALWAYS', 119),\n",
       " ('RECOMMEND', 12),\n",
       " ('SISTERS', 76),\n",
       " ('CHILDREN', 25),\n",
       " ('VEXING', 1),\n",
       " ('POOR', 38),\n",
       " ('NERVES', 4),\n",
       " ('HIGH', 17),\n",
       " ('OLD', 15),\n",
       " ('YEARS', 36),\n",
       " ('SINCE', 59),\n",
       " ('CAPRICE', 4),\n",
       " ('INSUFFICIENT', 10),\n",
       " ('UNDERSTANDING', 21),\n",
       " ('FANCIED', 7),\n",
       " ('NERVOUS', 4),\n",
       " ('VISITING', 5),\n",
       " ('WAITED', 7),\n",
       " ('ASSURING', 4),\n",
       " ('TILL', 93),\n",
       " ('EVENING', 74),\n",
       " ('AFTER', 200),\n",
       " ('FOLLOWING', 27),\n",
       " ('TRIMMING', 1),\n",
       " ('SUDDENLY', 16),\n",
       " ('ADDRESSED', 17),\n",
       " ('WE', 255),\n",
       " ('FORGET', 17),\n",
       " ('SHALL', 164),\n",
       " ('PROMISED', 21),\n",
       " ('TWO', 131)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "counts_test_2 = text.flatMap(clean_split_line).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "counts_test_2.take(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618d9dd",
   "metadata": {},
   "source": [
    "### Spark and Lazy Evaluation\n",
    "\n",
    "\n",
    "* Transformation on an RDD  are delayed until an action is performed\n",
    "\n",
    "  * Similar to python genertors\n",
    "\n",
    "  * This is called lazy evaluation\n",
    "\n",
    "* You can chain many transformations on the same RDD without causing any execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4a9b177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aeac4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following wo't return an error until an action is performed\n",
    "data = text.map(lambda x: len(x)/0).filter(lambda x: x>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1a17040c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 22:49:37 ERROR Executor: Exception in task 0.0 in stage 30.0 (TID 84)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_50/1492525221.py\", line 2, in <lambda>\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 22:49:37 ERROR Executor: Exception in task 1.0 in stage 30.0 (TID 85)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_50/1492525221.py\", line 2, in <lambda>\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 22:49:37 ERROR Executor: Exception in task 2.0 in stage 30.0 (TID 86)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_50/1492525221.py\", line 2, in <lambda>\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 22:49:37 ERROR Executor: Exception in task 3.0 in stage 30.0 (TID 87)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_50/1492525221.py\", line 2, in <lambda>\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 22:49:37 WARN TaskSetManager: Lost task 0.0 in stage 30.0 (TID 84) (804949680baf executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_50/1492525221.py\", line 2, in <lambda>\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "21/09/22 22:49:37 ERROR TaskSetManager: Task 0 in stage 30.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 84) (804949680baf executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_50/1492525221.py\", line 2, in <lambda>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_50/1492525221.py\", line 2, in <lambda>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50/724869387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# the `ZeroDivisionError: division by zero` is burried in many Scala error messages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 84) (804949680baf executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_50/1492525221.py\", line 2, in <lambda>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_50/1492525221.py\", line 2, in <lambda>\nZeroDivisionError: division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# The following will generate an error since the transformation dividing by 0 \n",
    "# is executed\n",
    "# the `ZeroDivisionError: division by zero` is burried in many Scala error messages.\n",
    "\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beeff92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c483f56",
   "metadata": {},
   "source": [
    "### The Spark Computation DAG\n",
    "\n",
    "* Lazy evaluation is possible because Spark maintains a graph (DAG) of the transformation transformations\n",
    "* The transformating are optimized and executed in the graph once an action is triggered\n",
    "\n",
    "* A simple exampe of an execution is:\n",
    "\n",
    "```python\n",
    "data_2 =  data_1.map(lambda x: x+2)\n",
    "# do some work here\n",
    "data_3 =  data_2.map(lambda x: x-2)\n",
    "```\n",
    "\n",
    "* The above transformations are not run because it does not change the value of `x`.\n",
    "  * `data_3` is equal to `data_1`\n",
    "\n",
    "* See the the following blog post about the catalyst optimizer.\n",
    "\n",
    "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n",
    "\n",
    "* More on this when we cover Spark SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a17abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
