{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5dc66ec",
   "metadata": {},
   "source": [
    "### Some NLP Tasks\n",
    "* Feature Extraction\n",
    "  * Getting the vector embedding of word, sentence, paragraph or even document\n",
    "* Question answering\n",
    "* Sentiment analysis\n",
    "* Summarization\n",
    "* Zero shot classification\n",
    "* Named entity recognition\n",
    "* Etc.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16af25",
   "metadata": {},
   "source": [
    "### Huggin Face Model Hub\n",
    "\n",
    "* The apple store of language models\n",
    "  * Some preliminary image models as well\n",
    "\n",
    "\n",
    "* Thousands of pre-trained models  \n",
    "  * Default models pretrained for specific tasks\n",
    "* Support for over 100 languages.\n",
    "\n",
    "* APIs to download and use those pre-trained models on a given text\n",
    " * Complete pipeline, including text processing\n",
    " * Often painfully difficult to process the data which is often model specific\n",
    "* Possibility to fine tune the model for custom data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c704bdb",
   "metadata": {},
   "source": [
    "### Model Hub Transformers\n",
    "\n",
    "* Transformers are an architecture (family) of language models\n",
    "  * In the same way that CNNs and RNNs are common architectures for working with image or sequential data. \n",
    "\n",
    " * Trained on large amounts of raw text in a self-supervised fashion. \n",
    "    * Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. \n",
    "\n",
    "* Resulting models develop statistical understanding of the language they has been trained on\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f91bece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3b07732e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999866783618927}]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"ICS 438 is an amazing course. Everyone should take it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "092f4938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.999622106552124}]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"What a super boring movie. Never goingt to recomment it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2d45cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_to_classify = [\n",
    "                            \"I really like the new design. Your app rocks!\",\n",
    "                            \"What a mess this site is to navigate\",\n",
    "                            \"very confusing to get anything done on this new redesign\"\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "7ece8ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998397827148438},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997925162315369},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996102452278137}]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(sentences_to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "aa393f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999119758605957}]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Positive?\n",
    "### Models are not humans -- undrstanding of the limitations is critical\n",
    "classifier(\"He went home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77947f",
   "metadata": {},
   "source": [
    "### Example: Zero Shot Classification\n",
    "\n",
    "* Zero-shot learning: solve a task despite not having received any training examples of that task.\n",
    "  * E.g.: recognizing a category of object in photos without ever having seen a photo of that kind of object before. \n",
    "  *  Needs to predict the class they belong to. \n",
    "\n",
    "\n",
    "* You can learn more about zero-shot learning in Sec. 15.2 of the Deep Learning textbook: http://www.deeplearningbook.org/contents/representation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c4cf11fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'The CDC is approving booster vaccine shots for everyone over the age of 18.',\n",
       " 'labels': ['healthcare', 'business', 'polics'],\n",
       " 'scores': [0.9048793315887451, 0.06191409006714821, 0.03320661932229996]}"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "classifier(\n",
    "    \"The CDC is approving booster vaccine shots for everyone over the age of 18.\",\n",
    "    candidate_labels=[\"politics\", \"business\", 'healthcare'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae06d7",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "* Find the entities (such as persons, locations, or organizations) in a sentence. \n",
    " * Classify each label of the sentence to a class per entity and one class for “no entity.”\n",
    "* Default classes\n",
    "    * O means the word doesn’t correspond to any entity.\n",
    "    * PER person entity\n",
    "    * ORG: organization entit\n",
    "    * LOC: location entity\n",
    "    * MISC: miscellaneous entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "4ff0c494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.996318,\n",
       "  'word': 'Elon Musk',\n",
       "  'start': 0,\n",
       "  'end': 9},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9502535,\n",
       "  'word': 'Tesla',\n",
       "  'start': 43,\n",
       "  'end': 48}]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"Elon Musk asked whether he should sell off Tesla stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc0b4f",
   "metadata": {},
   "source": [
    "### Tranforms are Big Models\n",
    "\n",
    "* Transformers are typically very large models\n",
    "\n",
    "![](https://miro.medium.com/max/1338/1*40VA19kG5zUmTj-AOnh47A.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce14d5",
   "metadata": {},
   "source": [
    "### Transformers: an Encoder and a Decoder\n",
    "\n",
    "![](https://miro.medium.com/max/923/0*L9Zx_5BBFSXgGvvx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72858d6c",
   "metadata": {},
   "source": [
    "### Transformers: an Encoder and a Decoder\n",
    "* The encoder finds an appropriate representation of the input\n",
    "* The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence.\n",
    "\n",
    "* It turns out that the encoder and decoder can be used idependently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a849a",
   "metadata": {},
   "source": [
    "### Transformers Architecture: The encoder\n",
    "\n",
    "* As discussed, Word2Vec can be used as a dictionary look to assign embeddings to words\n",
    "  * This is a problem since the meaning of a word depends on its' content\n",
    "  * e.g.: English language is full of homonyms \n",
    "    * \"He banks as FHB\" vs. \"He banks on her support to win the election\"\n",
    "    * \"He wore a bow tie to the event\"  vs. \"He used a bow and arrows to hunt the prey.\"\n",
    "\n",
    "* The encoder receives an input and builds a *contextualized* representation of it (its features). \n",
    "  * This means that the model is optimized to acquire understanding from the input.\n",
    "* One of the most popular encoder models is BERT\n",
    "  * Bidirectional Encoder Representations from Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953ce8a",
   "metadata": {},
   "source": [
    "### Transformers Architecture: Example Encoder -- BERT\n",
    "\n",
    "* BERT is a way to contextually encode a word\n",
    "  * The embedding of the word is context dependent. \n",
    "  * \"He banks as FHB\" vs. \"He banks on her support to win the election\"\n",
    "  * The value of \"banks\" takes into account the value of the words around it.\n",
    "    \n",
    "* Unlike Word2Vec, BERT does not just operate like a dictionary\n",
    "\n",
    "* Size of the generated embedding depends on the architecture\n",
    "  * For BERT base , the size 768\n",
    "\n",
    "* BERT embedding is said to hold the meaning within the text\n",
    "  * BERT tokenization, so not \"1 word = 1 embedding\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "92579746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "my_input = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "8606ea44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  8667,   117,  1139,  3676,  1110, 10509,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "f2164a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**my_input)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "15c4d6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9c288c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  8667,   117,  1139,  3676,  1110, 10509,   102]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e482ec",
   "metadata": {},
   "source": [
    "### More encoders\n",
    "\n",
    "* There are dozens of different encoder architectures. For example:\n",
    "    \n",
    "* Thre are also dozens or modes that fine-tune BERT to specific domains\n",
    " * FinBERT(Finance) https://github.com/ProsusAI/finBERT\n",
    " * med-BERT (medica field) https://github.com/ProsusAI/finBERT\n",
    " * Sci-BERT: Scientific Text Bert (https://aclanthology.org/D19-1371.pdf)\n",
    "  ...        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40bb49",
   "metadata": {},
   "source": [
    "### Transformers Architecture: The Decoder\n",
    "\n",
    "* The decoders work similarly to an encoder \n",
    " * Unlike the encoder, the decoder uses masked self-attention. \n",
    "   * Unlike the encoder, it can only see the words on one side (ex. left)\n",
    "\n",
    "* The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence.\n",
    "* Word at position $i$ depends only on words at positions $i-1$ \n",
    "  * This means that the model is optimized for generating outputs.\n",
    "* Decoders are not as relevant for this course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d638cfaf",
   "metadata": {},
   "source": [
    "### Popular Transformers\n",
    "\n",
    "* GPT and GPT-2: transformer-based language model (GPT-2 has 1.5 billion parameters)\n",
    "  * Trained on 8 million web pages\n",
    "* GPT-3, or the third generation GPT transormer \n",
    "  * 175 billion learning parameters\n",
    "  * Incredible (scary) good at a dizzying number of tasks\n",
    "  * Generating Web Component or SQL code from a language query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580f753",
   "metadata": {},
   "source": [
    "### The Pipeline Step in Details\n",
    "\n",
    "* Word Tokenization\n",
    "* Input processing\n",
    "* Models Processing\n",
    "* Post-processsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd375795",
   "metadata": {},
   "source": [
    "### Word Encoding\n",
    "\n",
    "* Analytics on text ues numerical values. \n",
    " * Various different strategies to infer those values\n",
    "* Native: asssign a unique value to each wor in the vocabulary\n",
    "   * {\"aardvark\": 1, ... \"Zeuxis\": 125,452}\n",
    "\n",
    "* This approach is referred to as work tokenization\n",
    "  * Split on words and ponctuation\n",
    "  * Assign each word a unique ID \n",
    "\n",
    "\n",
    "* Issues with this approach\n",
    "* Corpus may contain hundreds of thousads of words and dataset can be very large \n",
    "  * How de encode these without explicitly accountig for every word in the language?\n",
    "* Also, how do you encode for very similar words (car vs. cars, text vs. textual)\n",
    "  * Yes, we can rely on embedding to be fairly similar, but may be we can encode wors so that they match before computing embedding\n",
    " * How should a deployed model handle previously unseen words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea453afa",
   "metadata": {},
   "source": [
    "### Character Tokenization\n",
    "\n",
    "* The other end of the spectrum, we could encode every charcater independently\n",
    "  * Our encoder need only need valid alphabet and punctation characters\n",
    "* Small encoding scheme but can encode any word in the same alphabet\n",
    "\n",
    "Issue with this approach:\n",
    "\n",
    "* Encoding for single sentence become large\n",
    "* token do no mean anything when taken separately\n",
    "  * need to be combined to generate a userful meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c8555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ad07d0",
   "metadata": {},
   "source": [
    "### Sub-word Encoding: An Intuition\n",
    "\n",
    "* Language contain hundreds of thousads of words and text is often very big \n",
    "  * How de encode these without explicitly accountig for every word in the language?\n",
    "    \n",
    "* Use tokens insted of words\n",
    "  * Split a work into prefix stem and suffic\n",
    "  * unusually -> un + usual + ly  \n",
    "  * unsuspiciously -> un + suspicious + ly  \n",
    "\n",
    "*  [suspicious, usual, un, ly] with these 4 tokens  construct 8 words \n",
    "  * susupicious, usual unsuspicous, unusual, usually, suspivciously, unusually, unsuspiciously \n",
    "    \n",
    "\n",
    "\n",
    "  * makes it easy for words with similar stems to match while keeping the vocabulary small\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4019bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sub-word Encoding\n",
    "\n",
    "* Word shoud be split into meaningful subwords\n",
    "\n",
    "* Frequent words should not be split \n",
    "\n",
    "* Rarely used words should be split into subwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f2b5a",
   "metadata": {},
   "source": [
    "### Sub-word Tokenization Schemes\n",
    "\n",
    "* Different models use different schemes for splitting encoding words\n",
    "* Different approches and schemes\n",
    "  * Bert uses word piece: Tokenization introduced by Google.\n",
    "    * algorithm used has not been open sources; reverse engineered in some applications\n",
    "  * ALBERT uses Unigram \n",
    "    * Substantially different form Word Piece\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word Piece Algorithm\n",
    "* The word piece is a greedy algorithm\n",
    "* decomposes its' vocabulary into chunks and retains the most frequent ones\n",
    "* Builds a vocabulary containing the most frequent chunks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c654d79",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm -1 \n",
    "![](https://www.dropbox.com/s/q1dctybrlx2y0mh/wp_1.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd444dc",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm -2\n",
    "![](https://www.dropbox.com/s/f5626qe6mpdjapm/wp_2.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b9c52",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm -3\n",
    "![](https://www.dropbox.com/s/5m5n0qvn22lfwxk/wp_3.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc4184",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm - 4\n",
    "![](https://www.dropbox.com/s/4ol60txxkqy6evp/wp_4.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64a0cf",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm - 5\n",
    "![](https://www.dropbox.com/s/875ntqoyuhh8bg9/wp_5.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c1869",
   "metadata": {},
   "source": [
    "### Model to Tokenizer Mapping.\n",
    "\n",
    "* It's critical to use the correct encoding for each model we need to use. \n",
    "  * To use BERT model, we need to convert the input data using the same tokenizer it used for it's training\n",
    "    * split the workds in the same way that the model does\n",
    "    * Use the same delimiter characters\n",
    "    * use the same token ids as the model\n",
    "\n",
    "\n",
    "### For more details, see: https://huggingface.co/transformers/tokenizer_summary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30110632",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = tokenizer(\"Word tokenization is cool!\", return_tensors=\"pt\")\n",
    "my_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e21c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"##\" indicated the token does not occur at the start of the word.\n",
    "tokens = tokenizer.tokenize(\"Word tokenization is cool!\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337724bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c393bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input[\"input_ids\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tokenizer.vocab.keys())[10_000:10_020]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3128a0",
   "metadata": {},
   "source": [
    "### BERT Architecture and Embeddings\n",
    "![](https://jalammar.github.io/images/bert-output-vector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b10e2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Importing the relevant modules\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# Loading the pre-trained BERT model\n",
    "###################################\n",
    "# Embeddings will be derived from\n",
    "# the outputs of this model\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)# Setting up the tokenizer\n",
    "###################################\n",
    "# This is the same tokenizer that\n",
    "# was used in the model to generate\n",
    "# embeddings to ensure consistency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d55433f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text corpus\n",
    "##############\n",
    "# These sentences show the different\n",
    "# forms of the word 'bank' to show the\n",
    "# value of contextualized embeddings\n",
    "\n",
    "texts = [\"bank\",\n",
    "         \"The river bank was flooded.\",\n",
    "         \"The bank vault was robust.\",\n",
    "         \"He had to bank on her for support.\",\n",
    "         \"The bank was out of money.\",\n",
    "         \"The bank robber was arrested.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a333b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embeddings for the target\n",
    "# word in all given contexts\n",
    "target_word_embeddings = []\n",
    "\n",
    "for text in texts:\n",
    "    tokenized_input = tokenizer(text, return_tensors=\"pt\")\n",
    "    output = model(**tokenized_input)\n",
    "    embeddings  = output.last_hidden_state\n",
    "    embeddings = torch.squeeze(embeddings, dim=0)\n",
    "    word_index = tokenized_text.index('bank')\n",
    "    word_embedding = embeddings[word_index]\n",
    "    target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a0ff645e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bank</td>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>0.034353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bank</td>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>0.031118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bank</td>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>0.058298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bank</td>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>0.036416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bank</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.011138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>0.496597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>0.278664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>0.417891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.519588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>0.260012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>0.759213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.798656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>0.292080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.269614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.700466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text1                               text2  \\\n",
       "0                                 bank         The river bank was flooded.   \n",
       "1                                 bank          The bank vault was robust.   \n",
       "2                                 bank  He had to bank on her for support.   \n",
       "3                                 bank          The bank was out of money.   \n",
       "4                                 bank       The bank robber was arrested.   \n",
       "5          The river bank was flooded.          The bank vault was robust.   \n",
       "6          The river bank was flooded.  He had to bank on her for support.   \n",
       "7          The river bank was flooded.          The bank was out of money.   \n",
       "8          The river bank was flooded.       The bank robber was arrested.   \n",
       "9           The bank vault was robust.  He had to bank on her for support.   \n",
       "10          The bank vault was robust.          The bank was out of money.   \n",
       "11          The bank vault was robust.       The bank robber was arrested.   \n",
       "12  He had to bank on her for support.          The bank was out of money.   \n",
       "13  He had to bank on her for support.       The bank robber was arrested.   \n",
       "14          The bank was out of money.       The bank robber was arrested.   \n",
       "\n",
       "    distance  \n",
       "0   0.034353  \n",
       "1   0.031118  \n",
       "2   0.058298  \n",
       "3   0.036416  \n",
       "4   0.011138  \n",
       "5   0.496597  \n",
       "6   0.278664  \n",
       "7   0.417891  \n",
       "8   0.519588  \n",
       "9   0.260012  \n",
       "10  0.759213  \n",
       "11  0.798656  \n",
       "12  0.292080  \n",
       "13  0.269614  \n",
       "14  0.700466  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "list_of_sim= []\n",
    "for i in range(len(texts)-1):\n",
    "    for j in range(i+1,len(texts)):\n",
    "        text_1 = texts[i]\n",
    "        text_2 = texts[j]\n",
    "        embd_1 = target_word_embeddings[i]\n",
    "        embd_2 = target_word_embeddings[j]\n",
    "        cos_sim = 1 - cosine(embd_1.detach().numpy(), embd_2.detach().numpy())\n",
    "        list_of_sim.append([text_1, text_2, cos_sim])\n",
    "sims_df = pd.DataFrame(list_of_sim, columns=['text1', 'text2', 'distance'])\n",
    "sims_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d257a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The New York Times reported Tuesday that the FDA may authorize booster shots for all Americans \",\n",
    "    \"Apple rolling out new firmware update for AirPods and AirPods Pro headphones. Here’s how to check for it\",\n",
    "    \"Top infectious disease official said if more Americans get vaccines and booster shots, the disease could be downgraded to endemic status\",\n",
    "    \"Despite strong vaccination rates, Hawaii’s Safe Travels program likely isn’t ending anytime soon\",\n",
    "    \"Shopping Black Friday sales? Here are the best ways to pay\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ef38d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs= tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "outputs = model(**tokenized_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5ba24e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The New York Times reported Tuesday that the F...</td>\n",
       "      <td>Apple rolling out new firmware update for AirP...</td>\n",
       "      <td>0.965975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The New York Times reported Tuesday that the F...</td>\n",
       "      <td>Top infectious disease official said if more A...</td>\n",
       "      <td>0.985166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New York Times reported Tuesday that the F...</td>\n",
       "      <td>Despite strong vaccination rates, Hawaii’s Saf...</td>\n",
       "      <td>0.985678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The New York Times reported Tuesday that the F...</td>\n",
       "      <td>Shopping Black Friday sales? Here are the best...</td>\n",
       "      <td>0.825593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple rolling out new firmware update for AirP...</td>\n",
       "      <td>Top infectious disease official said if more A...</td>\n",
       "      <td>0.967587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apple rolling out new firmware update for AirP...</td>\n",
       "      <td>Despite strong vaccination rates, Hawaii’s Saf...</td>\n",
       "      <td>0.971793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apple rolling out new firmware update for AirP...</td>\n",
       "      <td>Shopping Black Friday sales? Here are the best...</td>\n",
       "      <td>0.815434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Top infectious disease official said if more A...</td>\n",
       "      <td>Despite strong vaccination rates, Hawaii’s Saf...</td>\n",
       "      <td>0.981738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Top infectious disease official said if more A...</td>\n",
       "      <td>Shopping Black Friday sales? Here are the best...</td>\n",
       "      <td>0.780993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Despite strong vaccination rates, Hawaii’s Saf...</td>\n",
       "      <td>Shopping Black Friday sales? Here are the best...</td>\n",
       "      <td>0.842483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  The New York Times reported Tuesday that the F...   \n",
       "1  The New York Times reported Tuesday that the F...   \n",
       "2  The New York Times reported Tuesday that the F...   \n",
       "3  The New York Times reported Tuesday that the F...   \n",
       "4  Apple rolling out new firmware update for AirP...   \n",
       "5  Apple rolling out new firmware update for AirP...   \n",
       "6  Apple rolling out new firmware update for AirP...   \n",
       "7  Top infectious disease official said if more A...   \n",
       "8  Top infectious disease official said if more A...   \n",
       "9  Despite strong vaccination rates, Hawaii’s Saf...   \n",
       "\n",
       "                                               text2  distance  \n",
       "0  Apple rolling out new firmware update for AirP...  0.965975  \n",
       "1  Top infectious disease official said if more A...  0.985166  \n",
       "2  Despite strong vaccination rates, Hawaii’s Saf...  0.985678  \n",
       "3  Shopping Black Friday sales? Here are the best...  0.825593  \n",
       "4  Top infectious disease official said if more A...  0.967587  \n",
       "5  Despite strong vaccination rates, Hawaii’s Saf...  0.971793  \n",
       "6  Shopping Black Friday sales? Here are the best...  0.815434  \n",
       "7  Despite strong vaccination rates, Hawaii’s Saf...  0.981738  \n",
       "8  Shopping Black Friday sales? Here are the best...  0.780993  \n",
       "9  Shopping Black Friday sales? Here are the best...  0.842483  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "list_of_sim= []\n",
    "for i in range(len(texts)-1):\n",
    "    for j in range(i+1,len(texts)):\n",
    "        text_1 = texts[i]\n",
    "        text_2 = texts[j]\n",
    "        embd_1 = outputs.pooler_output[i]\n",
    "        embd_2 = outputs.pooler_output[j]\n",
    "        cos_sim = 1 - cosine(embd_1.detach().numpy(), embd_2.detach().numpy())\n",
    "        list_of_sim.append([text_1, text_2, cos_sim])\n",
    "sims_df = pd.DataFrame(list_of_sim, columns=['text1', 'text2', 'distance'])\n",
    "sims_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffe98e",
   "metadata": {},
   "source": [
    "# How does BERT Work\n",
    "\n",
    "* See the followin excellent write up for ery detailed explaination of how BERT works. \n",
    "\n",
    "  * Warning: the scope is perhaps the levels of complexity are beyond the level of material introduced in this course.\n",
    "  \n",
    "BERT inner workings:  https://gmihaila.github.io/tutorial_notebooks/bert_inner_workings/#bertpooler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fafa3f",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "* Language models are typically trained on very large amounts of data.\n",
    "\n",
    "* Training can take weeks on very sophisticated architecture and cost very much $$$\n",
    "\n",
    "  * Training GPT-3 reportedly cost $12M for a single training run    \n",
    "  https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/\n",
    "\n",
    "* The training is done on generic data\n",
    "  * Note optimized for a specific domain (ex. healthcare or physics)\n",
    "* Training can also be done on a generic task \n",
    "  * e.g. Masked Language modeling\n",
    "  * training (\"my [MASK] is John Doe\", \"name\")\n",
    "  \n",
    "* An extremely powerful idea in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf23ae8",
   "metadata": {},
   "source": [
    "### Transfer Learning - cont'd\n",
    "\n",
    "* Transfer learning is the process of transferring knowledge from a model A, to a model B\n",
    "  * Model B may be doing a task that's slightly different\n",
    "    * E.g., model A does NER on news item and Model B does NER on invoices (company names, total-tax, item counts, etc..) \n",
    "    * Model A trained on English Wikipedia and model B on health care documents.\n",
    "    * Task can be different\n",
    "      * A does masked-language B does sentiment analysis\n",
    "* The knowledge acquired in model A is transferred to model B\n",
    "* Model B, typically needs a smaller dataset to be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64262ce",
   "metadata": {},
   "source": [
    "### Transfer Learning - cont'd\n",
    "\n",
    "* When training from scratch, all the model's weights are initialized randomly.\n",
    "* Transfer learning consists of \"continuing\" training with a new, smaller dataset\n",
    "  * Some approaches are used to force weights not to change too much.\n",
    "    * e.g., a much smaller learning rate or even freezing some layers and adding new ones that learn something new, \n",
    "* Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.\n",
    "    For the same reason, the amount of time and resources needed to get good results are much lower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c755307",
   "metadata": {},
   "source": [
    "### Pretraining in Language Model\n",
    "\n",
    "* In language models, pre-training is usually self-supervised\n",
    "  * Dataset does not require human annotation\n",
    "  * train on some task that allows the model to acquire some understanding of the language. Examples\n",
    "    * PRredict the next word in a sentence.\n",
    "      * This is, for example how GPT-2 was trained\n",
    "    * Masked Word Prediction\n",
    "      * BERT was trained on English Wikipedia and 11k books\n",
    "####### Next word prediction\n",
    "```\n",
    "Apple  -> will\n",
    "Apple will -> soon\n",
    "Apple will soon  -> allow\n",
    "Apple will soon allow -> customers\n",
    "Apple will soon allow customers -> to\n",
    "Apple will soon allow customers to -> fix\n",
    "Apple will soon allow customers to fix -> their\n",
    "Apple will soon allow customers to fix their -> devices\n",
    "Apple will soon allow customers to fix their devices -> .\n",
    "```\n",
    "####### Masked Word prediction\n",
    "```\n",
    "The company will [MASK] their earnings tomorrow.\n",
    "MASK= announce\n",
    "....\n",
    "```\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035bf80",
   "metadata": {},
   "source": [
    "### Transfer Learning for a Different Task\n",
    "\n",
    "![](https://www.dropbox.com/s/xt9io0croj9mked/transfer_learning.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48093ac",
   "metadata": {},
   "source": [
    "### Transfer Learning for a Different Task\n",
    "\n",
    "* The task should be as similar as possible\n",
    "  * Uses the same language\n",
    "  * Produce the same set of output (distributions over words)\n",
    "  * Etc.\n",
    "* The derived model leverages the language understanding acquired in the previous model\n",
    "    * Amazing, brilliant, fun, cool... convey somewhat similar meanings.\n",
    "    * When trained with \"such a fun movie\" -> \"positive\", it will learn that the sentence could have also been\n",
    "      * \"such an amazing movie\". \"such a brilliant movie\", \"such a cool movie\", etc..\n",
    "* Most NLP language will that language understanding acquired in the first layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662cd6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
